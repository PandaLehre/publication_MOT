\documentclass[lettersize,journal]{IEEEtran}
\usepackage{amsmath,amsfonts}
\usepackage{algorithmic}
\usepackage{algorithm}
\usepackage{array}
\usepackage[caption=false,font=normalsize,labelfont=sf,textfont=sf]{subfig}
\usepackage{textcomp}
\usepackage{stfloats}
\usepackage{url}
\usepackage{verbatim}
\usepackage{graphicx}
\usepackage{cite}

\usepackage{capt-of}  % <---
\usepackage{cuted}    % <===

% \hyphenation{op-tical net-works semi-conduc-tor IEEE-Xplore}
% 6 pages at all

% MD: Der größte Angriffspunkt für dieses Paper ist doch: Wir nutzen KITTI Tracks,
%	    damit können wir nie besser werden als die KITTI Tracks.
%		Die Tracks selbst sind aber mit einem Kalman Filter erstellt.
%		Damit kann das Prediction Network auch nie besser werden als ein Kalman filter.
%		Oder sehe ich das falsch?
% CH: Die KITTI Tracks wurden für das Training aus den GT Daten mit entsprechendem Rauschen erzeugt.
%		Daher ist kein direkter Einfluss von KF für die einzelnen Prädiktionen gegeben.
\begin{document}

% MD: Lass uns nochmal über den Titel sprechen. Können wir das nicht irgendwie positiv formulieren?
% 		Also das without ersetzen durch das, was wir machen.
% CH: with machine learning based predictions and soft association?
% 		\title{Multi-object tracking without dynamic models and hard association metrics}
\title{Multi-Object Tracking with Machine Learning based trajectories prediction
and soft track-association}

\author{Christian Alexander Holz, Christian Bader, Matthias Drüppel
	\thanks{C. Holz is with Daimler Truck AG, Research and Advanced Development, Stuttgart, Germany}
	\thanks{C. Bader is with Daimler Truck AG, Research and Advanced Development, Stuttgart, Germany}
	\thanks{M. Drüppel is with the Center for Artificial Intelligence,
		Duale Hochschule Baden-Württemberg (DHBW), Stuttgart, Germany}
}

% The paper headers
\markboth{Journal of \LaTeX\ Class Files,~Vol.~tbd, No.~tbd, tbd~2024}%
{Shell \MakeLowercase{\textit{et al.}}: A Sample Article Using IEEEtran.cls for IEEE Journals}

% \IEEEpubid{0000--0000/00\$00.00~\copyright~2021 IEEE}% Remember,
% if you use this you must call \IEEEpubidadjcol in the second% column
% for its text to clear the IEEEpubid mark.
\maketitle

% Unwritten rule: dont cite or reference figures or tables in the abstract
% MD: Das Abstract darf nicht zu lang werden, daher habe ich es gekürzt.
%		Ergebnisse mit auf nehmen ist aber genau richtig.
\begin{abstract}
% CH: Generiertes Abstract aus geschriebenem Abstract (I)
	This study introduces Machine Learning (ML)-based methodologies for Multi-Object Tracking (MOT)
	tailored to Advanced Driver Assistance Systems (ADAS), addressing the growing complexity and
	precision requirements in the automotive sector.
	We propose and evaluate three novel neural networks (NN) designed for MOT:
	(i) the Single Prediction Network (SPENT) for trajectory forecasting,
	(ii) the Single Association Network (SANT) for mapping single sensor inputs to existing tracks,
	and (iii) the Multi-Association Network (MANTa) for associating
	multiple sensor inputs to multiple tracks.
	These ML models are integrated with a traditional Kalman filter framework to enhance
	MOT performance while preserving the system's modularity and interpretability.
	Our evaluation demonstrates significant improvements:
	SPENT reduces the Root Mean Square Error (RMSE) by 50\% on the KITTI tracking dataset
	compared to a standard Kalman Filter; SANT and MANTa achieve a 95\% validation accuracy
	in sensor object assignment to tracks.
	These results highlight the opportunities of ML integration into tracking systems in enhancing performance and robustness while at the same time maintaining
	modularity and maintainability of ADAS tracking systems.

% CH: Gesschriebener Abstract (I)
	% In this paper, we develop Machine Learning (ML)-based methods for Multi-Object Tracking (MOT)
	% within the context of Advanced Driver Assistance Systems (ADAS).
	% Given the increasing complexity and demand for precise and efficient object tracking systems
	% in the automotive industry, this work focuses on the integration of ML techniques
	% into established tracking methodologies.
	% Key contributions encompass the creation and evaluation of three specialized neural networks:
	% (i) the Prediction Network for predicting the trajectories of tracked objects (SPENT),
	% (ii) the Single Association Network (SANT) for associating single incoming sensor object
	% to $n$ existing tracks, (iii) and the Multi-Association Network (MANTa)
	% for associating multiple sensor objects to the $n$ existing tracks.
	% We combine our ML methods with a traditional Kalman filter framework,
	% offering a data driven approach to address MOT challenges while maintaining
	% the modularity and interpretability of classical filter approaches.
	% We asses both, the performance of all three models alone as well as their
	% impact on the performance when they are integrated in the Kalman framework.
	% The results reveal a modular, robust, and maintainable tracker,
	% underscoring the potential of ML integration in ADAS Tracking Systems.

	% (i) Our results for the Prediction Network show a reduction in the
	% Root Mean Square Error (RMSE) on the KITTI tracking data set car by half
	% compared to a basic Kalman Filter. (ii) The Single Association Network was developed
	% as a replacement of the Global Nearest Neighbor method and enables
	% a purely data-based assignment method, achieving a validation accuracy of 95\% on KITTI cars.
	% (iii) The Multi-Association Network similarly achieves an accuracy of 95\%
	% on the same validation set, but enables an assignment of now $m$ sensor objects
	% to a set of $n$ tracks.

    % (i) Based on the KITTI tracking data set Car, the Root Mean Square Error (RMSE) for predictions could be reduced with our Prediction Network by half compared to a basic Kalman Filter.
    % The implementation allows the recurrent network to update the internal hidden states per time step, thus achieving an accurate state prediction without an external correction.
    % The network verification shows an RMSE of 0.026 averaged over the training, validation and test data set.
    % In relation to the predictions of the positions of an object in the X coordinate, this corresponds to an average deviation of 0.42m.

    % (ii) For data association, a Single Association Network was developed as a replacement for the classic Global Nearest Neighbor (GNN) method.
    % This means that SANT can replace the algorithms for calculating a distance metric and assignment procedures such as Hungarian Algorithm (HA) with the learned, training data-based assignment logic.
    % Based on a defined validation data (KITTI cars and vans) set with approx. 2700 samples, SANT achieves an accuracy of 95\%.

    % (iii) MANTa is a further development of SANT and addresses the limitation of individual assignment.
    % A network extension could be implemented that assigns a set of sensor objects $m$ to a set of tracks $n$.
    % The data situation was analysed in more detail and identified as a limitation for the network performance.
    % The verification carried out shows that MANTa achieves an assignment accuracy of 95\% in relation to the six most frequently occurring association sets.

\end{abstract}

% \begin{IEEEkeywords}
% 	Article submission, IEEE, IEEEtran, journal, \LaTeX, paper, template, typesetting.
% \end{IEEEkeywords}



%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%% chapter 1 %%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\section{Introduction}
%%%%%%%%%%%%%%%%%%%%%% Allgemeine Einführung MOT
\IEEEPARstart{T}{he}
ongoing evolution of Advanced Driver Assistance Systems has brought the need for precise
and reliable Multi Object Tracking into the spotlight
\cite{KF_Bewley.2016,KF_simple_cues.2022,KF_BoundingBD.2023,
DL_RNN_mot.2016,DL_RNN_data_association.2019,DL_CNN_ATT_mot.2017,DL_ATT_Trackformer.2022,
DL_ATT_CNN_soda.2020}.
In complex and dynamic environments, as encountered in urban traffic, it is crucial to
simultaneously and accurately capture the positions and movements of multiple objects
% CH: Kommentar von CB eingearbeitet
already in the early timesteps of detection with a accurate prediction - a key challenge in Computer Vision (CV) for automated driving.
In the commonly used Tracking-by-Detection (TbD) paradigm,
a tracker fuses detected sensor objects (SO) to create consistent object tracks over time.
A crucial step within this paradigm is the association of the incoming measured SO
with their corresponding existing object tracks to update their properties.
If no association can be made, new object tracks must be initialized.
%MD: Hier müssen noch einige Zitate rein.
Tracking frameworks form the heart of ADAS systems that are used in millions of vehicles
around the globe. The vast majority of these frameworks rely on classical approaches
such as the Kalman filter (KF) or its variants
\cite{KF_Bewley.2016,KF_simple_cues.2022,KF_BoundingBD.2023}.
These classical tracking theories have the great benefit of being modular and interpretable.
The task is split into clearly separated subtasks such as the prediction of currently
tracked objects and the association with newly measured ones.
Furthermore, the math and the theory itself is clean and comprehensible.
%MD: I added AD_Overview.2017, but I am not happy with the citation, there should be something better to be found. And it should best talk about the complexity that is introduced by applying the same software to different models. Maybe you can find something? Then I would cite it further down the paragraph
However, development for automated driving is highly complex \cite{AD_Overview.2017} and in the automotive industry tracking systems are not developed for a single model,
but rather as a platform software and are deployed to a variety of different vehicle models
with different sensor sets, different installation heights of the sensors,
used in different countries and must always perform for a wide range of driving scenarios.
This regularly leads to poor performance in certain scenes for a specific configuration.
With classical systems that not learn directly from data, these situations are often
solved by the implementation of heuristics and by tweaking parameters of the tracking system
for the troublesome scenes.
% CH: Kommentar von CB eingearbeitet
% MD: Finden wir für den nächsten Abschnitt noch ein Zitat? Wahrscheinlich nicht direkt ein Paper, aber vlt einen Bericht darüber, wie Automated Driving Entwicklung mit komplexen und vielseitigen Szenen umgehen muss.
But from a software engineering point of view hand-engineering parameters and relying on heuristics leads to a software that is extremely hard to maintain
and develop further for new scenarios and new system configurations.
Moreover, hand-engineered classical approaches can reach its limits for the overall
performance and in challenging situations that would require a more automated and
reproducible development strategy.

In this work, we propose a data driven approach to tracking frameworks,
which would allow the same system to be fine tuned for specific configurations relying only on data,
thus increasing maintainability and adaptability.
We do this preserving one of the biggest strengths of classical approaches: its modularity,
by replacing only single tracking components with ML models.

(i) In contrast to the Kalman filter \cite{KF_BoundingBD.2023}, our Prediction Network is capable
of predicting the state of individual objects without the need for a predefined state
or prediction model at runtime.
The self-learning, data driven approach enables adaptability to various scenarios and
the ability to effectively handle non-linearities and habits of road users.
Many conventional tracking systems rely on static methods for data association.
Commonly used algorithms like the Hungarian algorithm (HA) \cite{DA_hungarian.1955}
require heuristics and fixed thresholds.

(ii) Our Single Association Network replaces the calculation of a distance metric for
the corresponding assignment by employing machine learning
% CH: Kommentar von CB eingearbeitet
in order to resolve situations unclear for traditional approaches.
Both, the Prediction Network and SANT can be developed and evaluated as stand-alone models.
For a throughout evaluation, we proceed by integrating them into an existing tracking system
and demonstrate their performance through multiple tests and comparisons with established methods.
This work provides new insights and advancement in the development of ADAS by applying ML,
contributing to the further evolution of technologies for autonomous driving (AD).

%MD: Wenn wir schon zu (i) und (ii) was sagen, würde ich hier noch was zu (iii) also MANTa sagen, auch wenn es nur ein Satz ist.



%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%% chapter 2 %%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\section{Related work}
\subsection{State prediction for tracked objects}
One fundamental problem in Tracking-by-Detection frameworks is the prediction
of the states of the already tracked objects.
In many approaches Kalman filters and their variants have proven to be effective for
state prediction \cite{KF_simple_cues.2022, KF_Bewley.2016, KF_framework.2013}.
However, they reach their limits in more complex scenarios, particularly in the presence
of non-linear motion patterns and interactions among multiple objects
\cite{KF_Ristic.2004,KF_Julier.2004,KF_Wan.2000}. Ristic et al. \cite{KF_Ristic.2004} discuss the limitations of Kalman Filters in nonlinear and non-Gaussian
scenarios and introduces Particle Filters as an alternative solution.
Julier et al. \cite{KF_Julier.2004} propose the Unscented Kalman Filter (UKF) as an extension to the
standard Kalman Filter to handle non-linear motion models more effectively.
Wan et al. \cite{KF_Wan.2000} introduce the Gaussian Mixture Model for tracking multiple objects
in a cluttered environment.
In this work, we introduce a novel MOT approach that leverages Machine Learning
to overcome these challenges.
We specifically focus on the development and implementation of Neural Networks,
which can enable more precise and flexible data-driven object tracking.

\subsection{Association of sensor objects to tracks}
Another fundamental problem for a TbD tracker is the data association.
% CH: Kommentar von CB eingearbeitet
One of the most common methods for data association is the Global Nearest Neighbor (GNN) algorithm.
The GNN algorithm assigns the nearest detected object to the existing tracks based on a distance metric.
Although GNN is often implemented using the Hungarian Algorithm \cite{DA_hungarian.1955}
to find the optimal assignment between observations and tracks,
it only considers the current set of observations and does not explicitly
take temporal continuity or motion patterns into account.
This limitation can lead to incorrect assignments in complex scenarios,
especially when objects cross paths or when sensor measurements are noisy.
The Hungarian Algorithm considers the assignment problem as a linear sum assignment problem
taking into account the current state of the tracks and the sensor objects.
Other approaches, such as the Joint Probabilistic Data Association (JPDA) \cite{DA_JPDA.1993},
consider the temporal information of the tracks and the sensor objects.
However, these methods are computationally expensive and require the calculation of
a joint probability distribution for all possible assignments.
Machine Learning-based approaches have been proposed to address these limitations.
This aggregate of temporal information can be done for example using attention mechanisms
as used in \cite{DL_ATT_CNN_soda.2020, DL_ATT_CNN_mot_sot_based.2017} or
recurrent neural networks (RNNs) as developed in \cite{DL_RNN_mot.2016, DL_RNN_data_association.2019}.
The latter is what we are also pursuing in this work.
Similar to the problem statement by Mertz et al. \cite{DL_RNN_data_association.2019},
the aim of our work is to develop a data-based approach that can learn to
completely solve the combinatorial non deterministic polynomial time (NP)
hard optimization problem of data association.
% Mertz et al \cite{DL_RNN_data_association.2019} use a distance matrix based on the
% Euclidean distance measure as input for the developed association network,
% thus replacing an association algorithm such as the Hungarian Algorithm.
In the context of this work, we put forward the hypothesis that a Gated Recurrent Unit (GRU)-based
association network can be designed and trained using an undefined distance measure as is elaborated in the next chapter.

\subsection{Real time applications}
ADAS are embedded real-time applications where it is crucial to predict the state of objects
immediately after their detection as described in \cite{KF_Bewley.2016,DL_Wojke.2017}.
This rules out offline tracking methods as presented in \cite{offline_mot.2017} that process
the entire video material at once in a batch process.
Therefore, most recent approaches for tracking multiple objects rely on online methods
that do not depend on future image information.
Online methods use various features to estimate the similarity between the recognized objects
and the existing tracks.
This can be done on the basis of their predicted positions or even similarities in appearance.

\paragraph{Kalman Filter based tracker}
Our ML models are integrated into a Kalman Filter tracking system.
The Kalman Filter is a popular choice for a multi-object tracker due to its interpretability and performance \cite{KF_Wan.2000,KF_Ristic.2004,KF_Julier.2004,KF_framework.2013,KF_Bewley.2016,KF_simple_cues.2022,KF_BoundingBD.2023}
Bewley el al. \cite{KF_Bewley.2016} present a simple and efficient multi-object tracking approach
based on the KF and the Hungarian Algorithm.
Seidenschwarz el al. \cite{KF_simple_cues.2022} present a MOT approach based on simple visual cues.
The authors argue that many existing multi-object trackers are overly complex
and require significant computational resources.
Instead, they propose a more straightforward approach that leverages basic visual features
such as color, shape, and motion.
These cues are used to track objects at the image level and to make frame-to-frame associations.

\paragraph{Recurrent Neural Network based Tracker}
% Generierter Text aus geschriebenem Text (II)
Similar to our methodologies, studies by \cite{DL_RNN_mot.2016,DL_RNN_data_association.2019,
DL_RNN_data_association.2020} introduce RNN-based approaches for online multi-target tracking.
Specifically, the work by Mertz et al. \cite{DL_RNN_data_association.2019} focuses on
data association within a TbD framework.
Their proposed DeepDA model, an LSTM-based Deep Data Association Network, is designed to learn
and execute the task of associating objects across frames.
This model's ability to discern association patterns directly from data enables the achievement
of robust and reliable tracking outcomes, even in environments with significant disturbances.
Mertz et al. employ a distance matrix, derived from the Euclidean distance measure,
as the input for the DeepDA network.
This innovative approach effectively supersedes traditional association algorithms,
such as the Hungarian Algorithm.
It is inferred that the Euclidean distance measure served as a foundation not only for generating
the ground truth (GT) training data (i.e., distance matrices)
but also for the subsequent evaluation process.
% MD: hast du hier nachgefragt?
%		Der Satz klingt so nach "wir raten das jetzt mal"
% CH: Anfrage ist raus, Satz schon einmal etwas angepasst.
% MD: Ich habe es noch etwas angepasst, bitte nochmal drüber gucken. Passt aber sonst für mich.
However, this is not explicitly stated.
In our work, we want to enable the network to follow a completely data-driven association logic without forcing a concrete distance metric.

% CH: Geschriebener Text (II)
	% Similar to our methods \cite{DL_RNN_mot.2016,DL_RNN_data_association.2019,
	% DL_RNN_data_association.2020} present approaches for online multi-target tracking using RNNs.
	% The approach presented in my Mertz et al. \cite{DL_RNN_data_association.2019} focusses
	% on the task of data association in a TbD framework.
	% The developed DeepDA model represents an Long Short-Term Memory (LSTM)-based
	% Deep Data Association Network to learn and perform the association of objects between frames.
	% By learning association patterns from the data, the tracker can achieve robust and
	% reliable tracking results even in highly disturbed environments.
	% Mertz et al. use a distance matrix based on the Euclidean distance measure as input data
	% for the developed DeepDA network, thus replacing an association algorithm such as the
	% Hungarian Algorithm.
	% It can be assumed that the Euclidean distance measure was also used as the basis for
	% generating the ground truth (GT) training data (distance matrices) and for the evaluation.

%MD: Hier muss noch eine Verbindung zu unserem Paper rein!!
\paragraph{Attention Mechanism based Tracker}
Each of the papers \cite{DL_ATT_mot_sot_based.2017,
	DL_ATT_CNN_mot_sot_based.2017,
	DL_ATT_CNN_soda.2020,
	DL_CNN_ATT_mot.2017,
	DL_ATT_Trackformer.2022}
present approaches for a tracker that utilize the attention mechanism \cite{ML_Attention.2017},
for example to compute soft data association \cite{DL_ATT_CNN_soda.2020}.
The main research focus of the paper \cite{DL_ATT_CNN_soda.2020} is on soft data association,
which enables the tracker to make probabilistic associations between objects
and account for uncertainties in the associations.
Soft data association (SoDA) in the SoDA model works by using attention mechanisms
to aggregate information from all detections in a given temporal window.
This allows the model to learn long-term and highly interactive relationships between
detections and tracks from large datasets without using complex heuristics and hyperparameters.



%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%% chapter 3 %%%%%%%%%%%%%%%%%%%%%%%%%%%%%

%MD: Oben hast du es Prediction Network genannt, jetzt wieder SPENT. Wir sollten uns auf jeden Fall entscheiden.
\section{Overview of our proposed models}
Our primary contribution is the development and evaluation of three NN that we label:
(i) the Single Prediction Network (SPENT), (ii) the Single Association Network (SANT),
and (iii) the Multi-Association Network (MANTa).
Figure \ref{pic::MotFramework} provides an overview of our approach
in which the association network can be implemented using either SANT or MANTa.
The input for the proposed Prediction Network (i) are the sensor objects
$ Z_{t,1:m}$ in every time step.
If new objects are detected, these are stored in a $k$-dimensional state vector
containing information such as object position (x,y), yaw angle and object dimensions
(length and width) (for $k=5$).
The Prediction Network predicts all vectors $X_{t,1:n}$ to the next time step where
they are used as input to either the SANT or MANTa association networks.
These provide the association matrix $A_{t,1:m}$, that is used to update the tracks $Tu_{t,1:n}$
using the corresponding sensor objects or create new tracked objects.
The Track Management can then decide to delete tracks, that were not updated
for a specific amount of time and send out tracks $Tc_{t,1:n}$ to the next higher
software component that have been confirmed by sensor objects.

\begin{figure}[htbp]
	\centerline{\includegraphics[width=0.95\linewidth]{figs/MOT_Framework_Integration_SPENT_SANT.png}}
	\caption{Schematic representation of the two integrated networks (blue)
	in a tracking-by-detection framework.
	The Association Network can either be filled with our SANT (single)
	or MANTa (multi) association models.}
	\label{pic::MotFramework}
\end{figure}




%%%%%%%%%% MD bis hier hin habe ich gelesen und bearbeitet %%%%%%%%%%%%



%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%% chapter 4 %%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\section{Tracking with ML based prediction and association networks}
We apply the tracking-by-detection paradigm, in which a tracker fuses object detections
to generate object tracks that are consistent over time.
\cite{KF_framework.2013}, for example, provides a framework for analyzing
tracking approaches that follow the TbD paradigm. This was used accordingly in this study.
To enable our models to incorporate temporal information, we use LSTM and
bidirectional Long Short-Term Memory (BILSTM) networks.
Both for the prediction and the association (SANT) of the sensor objects to the existing tracks.


%%%%%%%%%%%%%%%%%%%%%% Absatztitel: SPENT
\subsection{Single Prediction Network (SPENT)}
% Generierter Text aus geschriebenem Text (III)
In our methodology, we leverage the hidden states of the LSTM layer as a dedicated information
repository for each object.
The Prediction Network is architected for open-loop operation, signifying that it forecasts
future state values based on data previously received.
Within the context of an online MOT process, this enables the network to prognosticate
the most probable subsequent state values by utilizing the state values received for
a specific sensor object's state vector.

As depicted in Figure \ref{pic::SpentArch}, the schematic illustration of the generic
structure of the Prediction Network elucidates how the architecture is adeptly designed
to address the challenges of real-time state prediction.
This representation highlights the strategic deployment of the LSTM layer for storing
and processing object-specific information, facilitating accurate and timely predictions
of object states.
% Geschriebener Text (III)
	% Our approach is to use the hidden states of the LSTM layer as an object-specific information storage.
	% The Prediction Network was designed for an open-loop application,
	% this means that the network predicts values for a future time step based on previously received data.
	% For use in an online MOT process, the network is therefore able to make a prediction
	% about the most likely next state values using the state values received
	% for a given state vector of a sensor object.

\begin{figure}[htbp]
	\centerline{\includegraphics[width=0.95\linewidth]{figs/SPENT_network_arch.png}}
	\caption{Schematic representation of the generic prediction network structure.}
	\label{pic::SpentArch}
\end{figure}

\paragraph{Network architecture}
% Generierter Text aus geschriebenem Text (IV)
The foundational layer of our Prediction Network is constituted by an LSTM layer,
wherein the internal states — known as hidden states — are dynamically updated
at each timestep in response to incoming measurement data.
This iterative updating mechanism facilitates a continuous correction throughout
the sequence of each track, thereby enhancing the predictive accuracy of the network.
The quantity of hidden units within this layer directly correlates to
the volume of information retained across timesteps, as illustrated in Figure \ref{pic::SpentArch}.
These hidden states are capable of encapsulating information from all preceding timesteps,
independent of the sequence's length, ensuring a comprehensive temporal understanding.
Subsequent to the LSTM layer, our architecture incorporates a batch normalization layer,
which standardizes the LSTM output prior to its transition to the following Relu layer.
This normalization significantly expedites the training process and fosters better
convergence by mitigating internal covariate shift, as supported by \cite{DL_batch_norm.2015}.
Following this, a Relu layer is employed to apply a non-linear threshold operation,
setting any input value below zero to zero. During training, a dropout layer is introduced
to randomly nullify input elements with a specified probability,
thereby imposing a regularization effect and preventing overfitting,
as detailed in \cite{DL_dropout.2014}.
The architecture culminates in a Fully Connected (FC) Layer, which amalgamates
the localized insights garnered by preceding layers.
The dimensionality of the final FC layer is meticulously aligned with
the number of response variables required by the output layer, as elucidated in \cite{DL_FC.2010}.

Our model’s loss function is based on the Mean Squared Error (MSE) metric,
which is calculated for each state value prediction.
The MSE quantifies the average squared discrepancy between the predicted
and actual target values.
We chose MSE because it provides a clear and direct measure of how closely our predictions
align with the true states,
making it an effective metric for optimizing the accuracy of our model's state predictions.

% Geschriebener Text (IV)
	% The first layer of our Prediction Network is formed by the LSTM layer.
	% The internal values (hidden states) of this layer are updated per time step based
	% on the received measurement data.
	% This updating process therefore provides a correction over the course of the sequence of each track.
	% The number of hidden units (cells) corresponds to the amount of information
	% that the layer remembers between time steps, as shown in Figure \ref{pic::SpentArch}.
	% The hidden states can contain information from all the previous time steps,
	% regardless of the sequence length.
	% The values of the hidden states of the LSTM layer are updated in each time step
	% based on the received measurement data and the hidden states of the past time step.
	% These updates therefore results in an internal correction over the course of the sequence.
	% %BATCHNORM
	% In our prediction network, we have placed a batch normalization layer directly after the LSTM layer.
	% This means that the output of the LSTM layer is normalized before being
	% passed on to the subsequent Relu layer.
	% This helped speed up training and improve convergence by reducing internal
	% covariate shifts \cite{DL_batch_norm.2015}.
	% %RELU
	% A Relu layer performs a threshold operation to each element of the input,
	% where any value less than zero is set to zero.
	% At training time, a dropout layer randomly sets input elements to zero with
	% a given probability providing a regularizing effect \cite{DL_dropout.2014}.
	% %FC
	% The Fully Connected (FC) Layer combines all of the local information learned by the previous layers.
	% The last FC layer must be equal to the number of response variables in the output layer
	% \cite{DL_FC.2010}.
	% %REGRESSION OUT
	% The basis for our loss is the mean-squared-error (MSE) calculated from the prediction
	% for each state value.
	% The MSE indicates the average of the squared difference between the model prediction
	% and the target value, which we use as the measure of the quality of the prediction.
	% For a single observation, the MSE is given by:

\begin{equation}
	MSE = \frac{1}{k}\sum_{i=1}^{k}{(y_i-\hat{y}_i)}^2,
\end{equation}
where $k$ is the size of the predicted state vector, $y_i$ is the ground truth value
(KITTI car tracks) and $\hat{y}_i$ is the prediction of the network for training set sample $i$.
The loss is evaluated for several sequences, each with numerous time stamps.
For our Prediction Network the loss function is half the mean-square-error
of the predictions added up for each time step in the training set,
normalized by the total number of all time stamps in the used sequences $T$.
The Factor 1/2 simplifies the calculation of the gradient during backpropagation,
making the derivative of the loss function with
respect to the network’s predictions easier to compute:
\begin{equation}
	loss = \frac{1}{2T} \sum_{j=1}^{T}\sum_{i=1}^{m}{({y_{ij}}-{\hat{y}_{ij}})}^2.
\end{equation}
During training, the average loss is calculated using the observations in the mini-batch,
so $T$ equals the mini-batch length.
% MD: Auf welchem Datensatz? Ich hoffe Validation oder?
% 		Training ist ziemlich egal.
%		Wenn wir nur die Position nehmen, dann hat der RMS sogar eine Einheit, nämlich m^2.
%		Du könntest noch die Wurzel ziehen, dann sind es Meter.
%		Aber dazu muss der Loss auf jeden fall NUR mit
%		den beiden x und y Koordinaten berechnet worden sein.
% CH: Stimmt, hatte ich für meine Präesentation schon verwendet.
%		Ergänze ich hier auch: (... Bezogen auf die Vorhersage der Y Koordinate
%		eines Objektes ergibt sich somit folgende durchschnittliche Abweichung SPENT 0.21m)

\paragraph{Data preprocessing}
In the development of our model, ground truth data comprising
vehicle tracks (cars and vans) from the KITTI dataset was utilized.
A GT track encapsulates the temporal evolution of an object,
delineating its trajectory from the moment it enters until it exits the sensor's detection range.
% MD: Wir haben nie über Datensplit gesprochen.
%		Das muss unbedingt rein.
%		Wie groß ist das Val set und wie wurde es erstellt?
%		Oder wird das direkt von Kitti geliefert, aber das dann auch schreiben.
%		Und zwar lange vor diesem Kapitel. Oder habe ich es selbst überlesen?
%		Ist aber total wichtig.
% CH: Hier eingefügt.
For preprocessing, a program was developed to extract all tracks of the selected object classes
(Cars and Vans) per sequence and store them in a unified dataset.
Through this iterative process, 635 tracks (Cars and Vans) were extracted from the entire KITTI dataset.
Tracks with a short temporal length were filtered using a threshold of 3 frames,
resulting in a final dataset comprising 624 tracks.
The track lengths vary from a minimum of 4 frames to a maximum of 643 frames (Sequence 20, ID 12).
To enhance model generalization and foster convergence during training,
we normalized the state values of tracks at time $t$ (predictors) and
at time $t+1$ (targets) in accordance with the methodology outlined in \cite{DL_book.2019}.
This normalization process aimed to standardize the distribution of both
predictors and targets to have a mean of zero and a unit variance.
The mean value $\mu$ and standard deviation $\sigma$ for each state variable
were computed across all tracks, employing the subsequent equations:
\begin{equation}
	\mu = \frac{1}{m} \sum_{i = 1}^{m} S_i
\quad\text{and}\quad
	\sigma = \sqrt{{\frac{1}{m - 1}} \sum_{i = 1}^{m} |S_i - \mu|^2}
\end{equation}
% MD: ... auch ist die Bezeichnung "length of all tracks" nicht genau genug.
%		Du meinst die Anzahl an time stamps? du meinst bestimmt nicht die Länge in Metern.
%		Die Normalisierung ist mir immer noch nicht klar.
% CH: Konsistenz (m für Länge State Vektor), T für time stamps pro Sequence.
%		Si über alle Sequenzen 1 bis i.
%		Formel für die Normalisierung aus: standard diviation
%		https://de.mathworks.com/help/matlab/ref/std.html
where $S_i$ the total number of all time stamps of all tracks
and $m$ is the number of states per track.

In our approach, we incorporate pre-padding as delineated by Reddy et al. in \cite{DL_padding.2019},
where the authors elucidate the effect of padding strategies on the performance of neural networks
in sequence-oriented tasks.
Their investigation reveals that while both pre-padding and post-padding are viable options,
the selection of padding technique plays a pivotal role in the model's efficiency.
This is particularly salient for LSTM networks, where the contextual integrity of the sequence
is paramount for optimal performance.
To handle sequences of varying lengths in our LSTM network, we utilized the padding technique.
In this process, shorter sequences are padded with special padding tokens so that all sequences
within a batch have the same length.

\begin{figure}[htbp]
	\centerline{\includegraphics[width=0.95\linewidth]{figs/padding2.png}}
	\caption{Analysis of Sequence Padding: Unsorted vs. Sorted Data.
			Delineates the differential impact of padding on LSTM network training,
			contingent upon whether the input data sequences are sorted by length.
			In the upper panel, unsorted data necessitates extensive padding to equalize
			the sequence lengths within a batch, thereby augmenting the computational overhead.
			Conversely, the lower panel illustrates that sorting data by sequence length prior
			to batching significantly curtails the requisite padding.}
	\label{pic::Padding}
\end{figure}

% MD: And what does this mean for us? You say it is crucial and then it is not described any further.
%		How did you apply padding?
% CH: added text and figure
% MD: Warum genau "one sequence adjustment" was heißt das?
% CH: ... a sequence adjustment ...
% MD: Bitte erkläre hier einmal was überhaupt padding in diesem Kontext heißt.
%		Du ergänzt Nullen am Anfang der Sequenz oder wie?
% CH: done

We used zeros as padding tokens, which were appended to the end of a sequence as needed.
This allows for efficient batch processing and ensures consistent training of the network
without the model performance being affected by the varying sequence lengths.
As highlighted by \cite{DL_padding.2019}, while padding introduces noise into the network,
it is essential for aligning sequences within each mini-batch
to facilitate the training of LSTM networks.
To mitigate this noise, we sorted the sequences in the training dataset by their length
prior to applying mini-batch padding.
As demonstrated in Figure \ref{pic::Padding}, this approach significantly reduces
the padding (depicted in turquoise) required for each mini-batch by utilizing a pre-sorted dataset.
This adjustment was critical for achieving convergence during the training process.
% MD: Die Unterschrift würde ich auch ausführlicher schreiben.
% CH: angepasst. Größe der Abbildung an Text / Seite angepasst.

% CH: Kommentar von CB eingearbeitet (Ergebnis ans Ende des Abschnitts verschoben)
Within the context of the KITTI dataset, encompassing both cars and vans,
our model attained a Root Mean Square Error of 0.029 for the positional prediction of all objects
within the validation dataset.
This performance metric translates to an average deviation of 0.38 meters for predictions
pertaining to the X coordinate and 0.21 meters for those related to the Y coordinate.
% MD: Das muss erklärt werden. Was heißt "standard Kalman Filter",
%		sonst hilft der Vergleich nicht viel.
%		Hier muss eigentlich nochmal der ganze Kalman Filter vorgestellt werden, mit dem du vergleichst.
% CH: wie besprochen ...inhouse implemented ...
Using an inhouse implemented Kalman Filter carried by Daimler Truck Research Group
following \cite{KF_BoundingBD.2023,KF_Bewley.2016}, an RMSE of 0.066 was achieved
on the same data set.


%%%%%%%%%%%%%%%%%%%%%% Absatztitel: SANT
\subsection{Single Association Network (SANT)}

The association network facilitates a data-driven methodology to address the combinatorial optimization
challenge of data association, which is known to be NP-hard.
Unlike traditional association methods referenced in
\cite{DL_RNN_mot.2016, DL_RNN_data_association.2019}, our SANT model innovates
by not relying on a predefined distance matrix as input.
Instead, it processes the extant tracks alongside each newly measured sensor object directly.
SANT is designed to match a singular sensor object to one of multiple track objects.
This approach obviates the necessity for a predefined distance metric,
thereby endowing the network with the autonomy to devise its own association logic,
which it learns from the training data.
Consequently, the association network supplants the traditional computation of a distance metric
and the implementation of an association algorithm, such as the
Hungarian Algorithm, with a data-driven, learning-based methodology.
% MD: Mit den Abkürzungen: SANT wird bestimmt eine Hand voll mal eingeführt.
% 		Eigentlich sollte man Abkürzungen genau einmal einführen und dann weiter verwenden.
%		Auch andere Abkürzungen werden mehrfach eingeführt.
%		Vlt würde ich es hier noch einmal im Titel einführen, aber im Text nicht mehr.
% CH: Abkürzungen zu Beginn eingeführt, im Text entfernt.
% 		Entweder Abkürzung oder ausgeschrieben, nicht beides.

\paragraph{Data preprocessing}
In the formulation of SANT, we conceptualized data association as a temporally structured challenge,
adopting a sequence-to-vector paradigm.
This framework posits the association of a singular sensor object, denoted as $Z_{(t,1)}$,
with a collection of tracks, represented as $X_{(t,1:n)}$.
These tracks were meticulously curated from the KITTI dataset,
which comprises annotated camera recordings.
It is imperative to note, however, that genuine ground truth data
for the specific association problem at hand are not available.
% MD: Der nächste Satz ist nicht zu verstehen :-D
% CH: besser? :D
To guarantee the unambiguity of assigning a sensor object to a single track within a specified
set of tracks, each sensor object was synthetically generated at a given timestep from
the pre-existing track set of that timestep.
Furthermore, to simulate realistic sensor data from the GT data, artificial noise was introduced.
This process was meticulously designed to reflect the inherent inaccuracies
and uncertainties present in real-world sensor measurements.
To achieve this, a maximum of 3\% of the value of the current state vector was randomly
subtracted or added to each value.
% MD: Das verstehe ich nicht. Warum mehrere Iterationen?
% CH: Somit konnten wir die Intensität des Rauschens in 7 Iterationen erhöhen
%		und die Datenmenge entsprechend erhöhen mit unterschiedlich verrauschten Tracks.
The data set was created in 7 iterations, so that the noise intensity was increased
by 0.5\% per iteration.
% CH: Anzahl an Positionen im State Vector m
As shown in figure \ref{pic::SantArch} the data format was created accordingly
to enable index-based track assignment for SANT.
The size of the input matrix therefore corresponds to $m \times n + 1$.
With $m = 5$ as the number of state values for our work and $n=16$
as the maximum number of tracks per time step.
% MD: Das stimmt so oder? n hängt ja nicht von t ab.
%		Die Dimension bleibt immer gleich, nur stehen eben manchmal Nullen drin.
%		Ich habe aber die Vermutung, dass du das anders meinst.
%		Warum sollen wir denn überhaupt ein n einführen für die Anzahl an nicht-null Tracks?
%		Das ist dem Netz doch egal. FÜr die Architektur ist nur die Max Anzahl wichtig.
% CH: Guter Punkt. Aus Netzwerksicht ist nur die Max Anzahl wichtig.
% MD: Ich habe den nächsten Absatz mal raus genommen.
% 		Was meinst du mit "in relation to"? Machen die das genau so? Dann schreib das konkret so.
% 		Das weiß ja keiner auswendig
% CH: Der KITTI Datensatz gibt die maximale Anzahl an Tracks pro Zeitstempel vor.
The actual number of tracks can vary between 0 and a maximum of 16 objects
in relation to the KITTI data set and the selected objects (cars and vans).
% MD: Normalization -> Batch Normalization
% CH: done
% MD: Oben steht concatenation of the "Prediction Network Output"
		% Das würde ich hier komplett weg lassen.
		% Das verwirrt, weil du es Output nennst,
		% es aber hier der Input für das vorgestellte Modell ist,
		% der zufällig noch der Output vom letzten Modell ist.
% CH: done

\paragraph{Network architecture}
The network as depicted in fig. \ref{pic::SantArch} is designed as a
sequence-to-classification network.
At each time step, a matrix is passed as input holding both,
the information of the one to-be-assigned sensor object
and the currently multiple tracked objects.

\begin{figure}[htbp]
	\centerline{\includegraphics[width=0.95\linewidth]{figs/SANT_network_arch.png}}
	\caption{Schematic representation of the generic network structure of the Single Association Network.}
	\label{pic::SantArch}
\end{figure}

%MD: Bitte diesen Absatz nochmal anschauen.
%		Der ist von der Struktur und von den Formulierungen noch sehr holprig.
%		Ich würde erst sagen was du ausprobiert hast und dann sagen was das beste war,
%		auch muss dann nur einmal erklärt werden was ein BILSTM layer ist.
% CH: angepasst
% MD: Darüber sollten wir nochmal sprechen: Bidirectional geht doch gar nicht für Tracking?
%		Du kannst doch nur von links nach rechts gehen oder wie sollte das laufen "online"?
% CH: Hier geht es um die Assoziation, nicht um die Vorhersage.
%		Es werden also alle Tracks pro Zeitschritt über das BILSTM betrachtet.
%		Es wird also als Sequenz betrachtet, welches SO welcehem Track zugeordnet wird.

Architectures with RNN, GNU, LSTM and Bidirectional
Long Short-Term Memory Network (BILSTM) layers were tested.
As part of these investigations, the best association performance was achieved with the BILSTM layer.
The network shown in Figure \ref{pic::SantArch} achieved the best performance
in comparison with a Training Accuracy of 95\% and Validation Accuracy of 95\%.
By combining the outputs of two LSTM layers that pass the information in opposite directions,
\cite{DL_LSTM.1997} demonstrates the ability to capture the context from both ends of the sequence.
The resulting architecture is called BILSTM.
% MD: Was heißt "last". Bitte erklären oder anders umschreiben.
% CH: angepasst
The output mode has been configured in the BILSTM layer, so that the layer
is able to receive a sequence as input and output value vector.
This form of dimension reduction is necessary in order to carry out a corresponding classification.
% FC SOFTMAX STACK
The last FC layer specifies the number of classes via the number of output values.
The classes are calculated in the softmax layer by applying the softmax function resulting
in a probability distribution.
The softmax function converts a number of values $z_i$ into a probability vector with $i$ values.
% MD: was ist die cross-entropy operation?
%		Du meinst du berechnest den Loss über die cross-entropy loss functions.
%		So würde ich das schreiben. Und das macht nicht der Output Layer,
%		sondern dafür nimmt man die Predictions des Models.
% CH: angepasst
%MD: Die Formel ist die binary Cross Entropy. Die gilt nur für genau zwei Klassen.
%		Das kann also hier nicht benutzt worden sein. Bitte nochmal nach schauen.
% 		Auch weiß ich nicht genau, warum manchmal große und manchmal kleine Buchstaben genutzt werden.
% 		Das sollte einer klaren Regel entsprechen.
% CH: done, plus Text umgeschrieben
The cross-entropy cost function is utilized to quantify the discrepancy between the network's
probabilistic predictions and the ground truth values,
a method particularly suited for tasks involving categorically exclusive classes.
This approach employs one-hot encoding to transform class representations
into binary vector formats, thus enabling the delineation of each class within a 1-to-n coding scheme.
The cross-entropy loss for each prediction, relative to its corresponding target value,
is computed as follows:
\begin{equation}
    loss = -\frac{1}{N} \sum_{i=1}^{N} \sum_{j=1}^{C} T_{ij} \log(Y_{ij}),
\end{equation}
where $N$ denotes the total number of samples, $C$ represents the number of class categories,
$T_{ij}$ is the GT indicator for whether class $j$ is the correct classification for sample $i$,
and $Y_{ij}$ is the predicted probability that sample $i$ belongs to class $j$,
as derived from the softmax function output.
% MD: ================ Bis hier habe ich gelesen MD ===============


%%%%%%%%%%%%%%%%%%%%%% Absatztitel: MANTa
% Generierter Text aus geschriebenem Text (V)
\subsection{Multi-Association Network (MANTa)}
The development of the single association network demonstrated that data-driven association logic
can be effectively learned by a deep learning model.
Building upon this foundation, the objective was to create a network capable of associating
multiple sensor objects ($m$ SOs) with multiple tracks ($n$ tracks) in a single operational step.
Unlike the Single Association Network, MANTa is specifically designed to handle the simultaneous
association of multiple sensor objects to multiple tracks.
To address this complex task, the Multi-Association Network (MANTa) was developed
to solve the following association scenarios:

\begin{itemize}
    \item \textbf{1 to n} - one sensor object (SO) with $n$ tracks
    \item \textbf{m to 1} - $m$ sensor objects (SOs) with one track
    \item \textbf{m to n} - $m$ sensor objects (SOs) with $n$ tracks
    \item \textbf{m to 0} - $m$ sensor objects (SOs) with no tracks
    \item \textbf{0 to n} - no sensor objects (SOs) with $n$ tracks
    \item \textbf{0 to 0} - no sensor objects (SOs) with no tracks
\end{itemize}

This comprehensive framework enables MANTa to adeptly manage the complexities inherent in
multiple object tracking (MOT), providing a robust and scalable solution for real-world applications.
% Geschriebener Text (V)
	% \subsection{Association network for multiple sensor objects (MANTa)}
	% The developed single association network shows that a data-based association logic
	% can be learnt from a deep learning model.
	% The aim was also to develop a network to recognise a number of $m$ sensor objects
	% to an existing number of $n$ tracks in one operation step.
	% In contrast to the Single Association Network, MANTa is designed to associate
	% multiple sensor objects to multiple tracks in one operation step.
	% Therefore the multi-association network was developed, to solve the following association problems:

	% \begin{itemize}
	% 	\item \textbf{1 to n} - one SO to $n$ tracks
	% 	\item \textbf{m to 1} - $m$ SO to one track
	% 	\item \textbf{m to n} - $m$ SO to $n$ tracks
	% 	\item \textbf{m to 0} - $m$ SO to no tracks
	% 	\item \textbf{0 to n} - no SO to $n$ tracks
	% 	\item \textbf{0 to 0} - no SO to no tracks
	% \end{itemize}

With the integration of MANTa into a Multi-Object Tracking framework,
the question can be asked whether a 0 to n and 0 to 0 assignment is a task to be solved.
If no new SO is detected, the prediction of the last operation step can be continued until
the track is deleted on the basis of the decreasing probability of existence
within the track management module.
The association algorithm or the MANTa does not need to be called if no tracks
and sensor objects are detected.
Although these assignment options can therefore be resolved via the programme structure
in the MOT framework, these options are also taken into account.
This is intended to ensure that the network also learns to deal with SOs and tracks
that are no longer available and capture information over time when no data is available.

\paragraph{Data preprocessing}
The data set for the training and validation of MANTa was created according to
the described objective.
Figure \ref{pic::MantaData} shows the input data structure with corresponding
association tasks in the displayed time step (85) of sequence 20 of the KITTI data set.
The respective tracks per time step were extracted across all sequences.
The extracted tracks were each modified with noise as in the SANT development and then normalised.
Figure \ref{pic::MantaData} shows the non-noisy sensor objects to enable
a visual assignment and increase understanding of the association procedure.
The values obtained in this way were assigned as shown in Fig. \ref{pic::MantaData} as measurements in pseudo-random order per time step assigned to an $F * T_{max}$ matrix.
Where $F$ represents the number of features.
The number of features results from the status values per track and SO set.
(here, $F = 2 * m$, with $m=5$ (number of state values in this investigation)).
$T_{max}$ stands for the maximum number of existing tracks per time step.
For the defined test case with cars and vans, the KITTI data set results in $T_{max}=16$.
There are 7 tracks in the time step of the sequence shown.
Each track receives a new measurement in this time step,
and an additional object was detected (new sensor object).
The GT assignment is displayed at the bottom of the section.
The size of $OH_{max}=288$ results from the maximum number of tracks $T_{max}=16$
and the number of possible assignment classes $C=18$.

\begin{strip} % <--- defined in "cuted"
	\includegraphics[width=\linewidth]{figs/MANTa_data.png}
	\captionof{figure}{MANTa, data structure, shows the non-noisy sensor objects to enable a visual assignment and increase understanding of the association procedure.}
	\label{pic::MantaData}
\end{strip}

The assignment classes result from the described index class 1 to 16 and additional degrees of freedom.
One degree of freedom of the assignment represents the case that no measurement exists,
another that the measurement should not be assigned.
The section of the one-hot vector shown (1:18) thus shows the GT assignment
of the first sensor object to the track at position two.

\paragraph{Network architecture}
The schematic representation \ref{pic::MantaArch} shows the developed network architectures
for the simultaneous association of a large number of sensor objects to a large number of tracks.
This is what we call a Multi-Association Network.

\begin{figure}[htbp]
	\centerline{\includegraphics[width=0.95\linewidth]{figs/MANTa_network_arch.png}}
	\caption{Schematic representation of the generic network structure of MANTa.}
	\label{pic::MantaArch}
\end{figure}

The BILSTM layer processes the input data as already explained for SANT.
The task of associating a sensor object list with a track list requires a
separate network part for each track.
This extension is labelled accordingly in the graphic \ref{pic::MantaArch}.
For each track (from 1 to $T_{max}$), the MANTa has been developed with
the familiar Fully Connected, Softmax stack.
Each softmax output consists of a vector with $C=18$ elements,
which represents the most probable assignment.
This means that a single assignment can be realised for each track.
The vectors 1 to $T_{max}$ are linked together in the Concatenation Layer.
This creates a vector with 288 elements, whereby 18 elements each represent
the most probable assignment of a measurement to a track.
The cost function was implemented according to the format of the GT and output data of the network.
The cross-entropy loss function already introduced was used and a clear assignment was realised
by means of additional iteration per time step through the respective track blocks.
The following formula is used to calculate the cross-entropy loss values for each
input value $Y_{i}$ and associated target value $T_{i}$ element by element.
To obtain a scalar per time step $loss_k$, all loss values are summed up and
divided by the number of classes $C$.
This results in the average loss per time step for all tracks.
\begin{equation}
	loss_k = \frac{1}{C} \sum_{i=1}^{C} -(T_i ln Y_i + (1 - T_i) ln(1 - Y_i))
\end{equation}
Then, all scalars obtained per time step are summarised and divided by the number of samples $N$ of a minibatch:
\begin{equation}
	loss = \frac{1}{N} \sum{} loss_k
\end{equation}

Using the described procedure, the Multi-Association Network (MANTa) was trained
to assign a list of sensor objects (SOs) to a list of tracks in a single operational step.
MANTa was trained on the same dataset as the Single Association Network (SANT)
and achieved an assignment accuracy of 95\% for the six most frequently
occurring association sets.
A detailed analysis of the data is presented in the following section.
% CH: Kommentar von CB eingearbeitet (Experimental Evaluation für alle oder keine Networks)
% \section{Experimental Evaluation}
In the context of the entire KITTI dataset, which includes both Cars and Vans objects,
MANTa achieves an average allocation accuracy of 70\%.
This limitation was primarily attributed to the characteristics of the extracted data.
As illustrated in Figure \ref{pic::MantaTracks}, the distribution of the number
of existing tracks per time step reveals significant insights.
Notably, time steps containing at least one track constitute nearly one-third
of the entire dataset, accounting for 29.9\% of the data,
which corresponds to an absolute count of 2315 samples.

\begin{figure}[htbp]
	\centerline{\includegraphics[width=0.95\linewidth]{figs/MANTa_tracks.png}}
	\caption{KITTI Cars and Vans diagramm, distribution of available data, number of tracks per sample}
	\label{pic::MantaTracks}
\end{figure}

% CH: Kommentar von CB eingearbeitet (MANTa advantage)
Time steps containing one to six tracks constitute 81.5\% of the samples.
Consequently, tests were conducted using a reduced dataset to demonstrate
the multi-association capability of the network.
MANTa correctly assigns 95\% of the dataset for time steps containing one to six tracks.
This result confirms MANTa's proficiency in handling data with the appropriate dataset,
as SANT also achieves a validation accuracy of approximately 95\%
across the entire KITTI dataset (including cars and vans).
The primary advantage of MANTa over SANT is its ability to assign
multiple sensor objects to multiple tracks in a single operational step.



%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%% Conclusion %%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\section{Conclusion}
% Generierter Text aus geschriebenem Text (VI)
The developed networks can be modularly integrated into the Tracking-by-Detection (TbD)
framework, effectively replacing classical heuristic algorithms within the
Multi-Object Tracking (MOT) process.

SPENT substitutes the state predictions of the Kalman Filter (KF).
The trained network estimates predictions per time step without the necessity
of a dynamics model.
This implementation allows the recurrent network to update its internal
hidden states at each time step,
thereby achieving accurate state predictions without external correction.
The model is suitable for real-time applications and represents a viable alternative
to classical prediction methods.
Network verification demonstrates a Root Mean Square Error (RMSE) of 0.026,
averaged over the training, validation, and test datasets.
Specifically, for the X coordinate position predictions of an object,
this corresponds to an average deviation of 0.42 meters.
For the Y coordinate position relative to the ego vehicle,
the average deviation is 0.23 meters.

For another primary task of the TbD MOT method, data association,
SANT was developed as a replacement for the
classical Global Nearest Neighbor (GNN) method.
SANT can replace algorithms for calculating a distance metric and
assignment procedures such as the Hungarian Algorithm (HA) with a learned,
data-driven assignment logic.
Based on a defined validation dataset with approximately 2700 samples,
SANT achieves an accuracy of 95\%.
MANTa is an advancement of SANT and addresses the limitation of individual assignment.
This network extension is capable of assigning a set of sensor objects ($m$)
to a set of tracks ($n$).
Detailed data analysis identified limitations affecting network performance.
Verification results indicate that MANTa achieves an assignment accuracy of 95\%
for the six most frequently occurring association sets.

% Geschriebener Text (VI)
	% The developed networks can be modularly integrated into the TbD framework and thus replace classic heuristic algorithms within the MOT process.

	% SPENT replaces the state predictions of the KF.
	% The trained network estimates the predictions per time step without the need for a dynamics model.
	% The implementation allows the recurrent network to update the internal hidden states per time step, thus achieving an accurate state prediction without an external correction.

	% The model is suitable for use in real time applications and represents an alternative approach to classical prediction methods.
	% The network verification shows an RMSE of 0.026 averaged over the training, validation and test data set.
	% In relation to the predictions of the positions of an object in the X coordinate, this corresponds to an average deviation of 0.42m. In relation to the position in the Y direction relative to the ego vehicle, the average deviation of the verification performed is 0.23m.

	% For another main task of the TbD MOT method, data association, SANT was developed as a replacement for the classic GNN method.
	% This means that SANT can replace the algorithms for calculating a distance metric and assignment procedures such as HA with the learned, training data-based assignment logic.
	% Based on a defined validation data set with approx. 2700 samples, SANT achieves an accuracy of 95\%.

	% MANTa is a further development of SANT and addresses the limitation of individual assignment.
	% A network extension could be implemented that assigns a set of sensor objects $m$ to a set of tracks $n$.
	% The data situation was analysed in more detail and identified as a limitation for the network performance.
	% The verification carried out shows that MANTa achieves an assignment accuracy of 95\% in relation to the six most frequently occurring association sets.



%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%% acknowledgments %%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\section*{Acknowledgments}
We would like to express our sincere thanks to the organisations that provided
financial support for this research project.
In particular, we would like to thank Daimler Truck AG and the DHBW Stuttgart.


%MD: Ich würde das hier nicht mit ins Paper aufnehmen. Für Abschlussarbeiten ist das durchaus üblich, aber bei Papern eher nicht.


% Without this support, our work would not have been possible.
% Our special thanks also go to our academic supervisors and mentors.
% I would like to thank Prof Dr Matthias Drüppel for his tireless support,
% valuable advice and continuous guidance throughout the duration of this project.
% His expertise and commitment have contributed significantly to the success
% of our research.
% Further thanks go to my colleague.
% I would like to thank Christian Bader for his essential help in conducting
% the experiments and providing technical resources.
% His support and fruitful discussions have greatly enriched this work.
% Finally, we would like to thank our families and friends for their unconditional
% support and encouragement.
% Their patience, understanding and motivation have seen us through the challenges
% of this project and have been an invaluable help.




%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%% BIB %%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\bibliographystyle{IEEEtran}
\bibliography{IEEEabrv, lit.bib}

\vfill

\end{document}


