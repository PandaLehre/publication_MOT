
\documentclass[lettersize,journal]{IEEEtran}
\usepackage{amsmath,amsfonts}
\usepackage{algorithmic}
\usepackage{algorithm}
\usepackage{array}
\usepackage[caption=false,font=normalsize,labelfont=sf,textfont=sf]{subfig}
\usepackage{textcomp}
\usepackage{stfloats}
\usepackage{url}
\usepackage{verbatim}
\usepackage{graphicx}
\usepackage{cite}
\usepackage{enumerate}
\usepackage{float}
\usepackage{stfloats}
\usepackage{acro}

\usepackage{capt-of}
\usepackage{cuted}

%------ for comments
\usepackage{color}
\newcommand{\menz}[1]{ \noindent {\color{red} [{\bf Markus:} {#1}]} }
\newcommand{\ch}[1]{ \noindent {\color{blue} [{\bf Christian:} {#1}]} }
\newcommand{\md}[1]{ \noindent {\color{green} [{\bf Matthias:} {#1}]} }
%------

% \input{acronyms}
% acronyms.tex
\DeclareAcronym{ADAS}{
    short = ADAS,
    long = Advanced Driver Assistance Systems
}
\DeclareAcronym{MOT}{
    short = MOT,
    long = Multi-Object Tracking
}
\DeclareAcronym{KF}{
    short = KF,
    long = Kalman Filter
}
\DeclareAcronym{SO}{
    short = SO,
    long = Sensor Object
}
\DeclareAcronym{SPENT}{
    short = SPENT,
    long = Single-Prediction Network
}
\DeclareAcronym{SANT}{
    short = SANT,
    long = Single-Association Network
}
\DeclareAcronym{MANTa}{
    short = MANTa,
    long = Multi-Association Network
}
\DeclareAcronym{RNN}{
    short = RNN,
    long = Recurrent Neural Network
}
\DeclareAcronym{LSTM}{
    short = LSTM,
    long = Long Short-Term Memory
}
\DeclareAcronym{BILSTM}{
    short = BILSTM,
    long = Bidirectional Long Short-Term Memory
}
\DeclareAcronym{GRU}{
    short = GRU,
    long = Gated Recurrent Unit
}
\DeclareAcronym{GT}{
    short = GT,
    long = Ground Truth
}
\DeclareAcronym{NP}{
    short = NP,
    long = Non-deterministic Polynomial Time
}
\DeclareAcronym{GNN}{
    short = GNN,
    long = Global Nearest Neighbor
}
\DeclareAcronym{JPDA}{
    short = JPDA,
    long = Joint Probabilistic Data Association
}
\DeclareAcronym{HA}{
    short = HA,
    long = Hungarian Algorithm
}
\DeclareAcronym{SoDA}{
    short = SoDA,
    long = Soft Data Association
}
\DeclareAcronym{FC}{
    short = FC,
    long = Fully Connected
}
\DeclareAcronym{MSE}{
    short = MSE,
    long = Mean Squared Error
}
\DeclareAcronym{ML}{
    short = ML,
    long = Machine Learning
}
\DeclareAcronym{NN}{
    short = NN,
    long  = Neural Network
}
\DeclareAcronym{TbD}{
    short = TbD,
    long  = Tracking-by-Detection
}
\DeclareAcronym{RMSE}{
    short = RMSE,
    long  = Root Mean Square Error
}

\begin{document}
\title{Data-Driven Object Tracking: Integrating Modular Neural Networks into a Kalman Framework}

\author{Christian Alexander Holz, Christian Bader, Markus Enzweiler, Matthias Drüppel
	\thanks{C. Holz is with Daimler Truck AG, Research and Advanced Development, Stuttgart, Germany}
	\thanks{C. Bader is with Daimler Truck AG, Research and Advanced Development, Stuttgart, Germany}
	\thanks{M. Enzweiler is with the Institute for Intelligent Systems, University of Applied Sciences, Esslingen, Germany}
	\thanks{M. Drüppel is with the Center for Artificial Intelligence,
    Baden-Württemberg Cooperative State University (DHBW), Stuttgart, Germany}
}

\markboth{Journal of \LaTeX\ Class Files,~Vol.~tbd, No.~tbd, tbd~2024}%
{Shell \MakeLowercase{\textit{et al.}}: A Sample Article Using IEEEtran.cls for IEEE Journals}

\maketitle

\begin{abstract}
This paper presents novel \ac{ML} methodologies for \ac{MOT}, specifically designed to meet the increasing complexity and precision demands of \ac{ADAS}. We introduce three \ac{NN} models that address key challenges in MOT: (i) the \ac{SPENT} for trajectory prediction, (ii) the \ac{SANT} for mapping individual \ac{SO} to existing tracks, and (iii) the \ac{MANTa} for associating multiple SOs with multiple tracks. These models are seamlessly integrated into a traditional \ac{KF} framework, maintaining the system’s modularity by replacing relevant components without disrupting the overall architecture.

Our evaluation, conducted on the public KITTI tracking dataset, demonstrates significant improvements in tracking performance. SPENT reduces the Root Mean Square Error (RMSE) by 50\% compared to a standard KF, while \ac{SANT} and MANTa achieve 95\% accuracy in sensor object-to-track assignments. These results underscore the effectiveness of incorporating task-specific NNs into traditional tracking systems, boosting performance and robustness while preserving modularity, maintainability, and interpretability.


\end{abstract}

\section{Introduction}
\IEEEPARstart{T}{he}
ongoing evolution of Advanced Driver Assistance Systems has brought the need for precise and reliable Multi Object Tracking into the spotlight {\cite{KF_Bewley.2016,KF_simple_cues.2022,KF_BoundingBD.2023, DL_RNN_mot.2016,DL_RNN_data_association.2019,DL_CNN_ATT_mot.2017,DL_ATT_Trackformer.2022, DL_ATT_CNN_soda.2020}}.
In complex and dynamic environments, as encountered in urban traffic, it is crucial to accurately capture and predict the positions of multiple objects already in the early time stamps of detection -- a key challenge in automated driving.
In the commonly used \ac{TbD} paradigm, a tracker fuses detected \ac{SO} to create consistent object tracks over time.
A crucial step within this paradigm is the association of the incoming measured \ac{SO} with their corresponding existing object tracks to update their properties.
If no association can be made, new object tracks must be initialized {\cite{KF_Bewley.2016,KF_simple_cues.2022,KF_BoundingBD.2023,OD_kampker.2018,OD_wu.2022}}.
Tracking frameworks form the heart of ADAS that are used in millions of vehicles around the globe.
The vast majority of these frameworks rely on classical approaches such as the \ac{KF} or its variants {\cite{KF_Bewley.2016,KF_simple_cues.2022,KF_BoundingBD.2023}}.
These classical tracking theories have the great benefit of being modular and interpretable.
The task is split into clearly separated subtasks such as the prediction of currently tracked objects and the association with newly measured ones.
However, development for automated driving is highly complex~{\cite{AD_Overview.2017}}.

In the automotive industry, tracking systems are typically developed as platform software designed to support a range of vehicle models, each of which may have varying sensor placements, system configurations, or even entirely different sensor suites. These systems must perform reliably across a wide spectrum of driving scenarios, which can introduce performance challenges in specific situations. Traditional solutions often rely on heuristics and manual parameter tuning, making the software cumbersome to maintain and difficult to extend. Moreover, these hand-engineered methods lack automated optimization, resulting in suboptimal performance in complex driving conditions. To address these limitations, we propose a data-driven tracking framework that allows for fine-tuning for specific configurations, thereby improving both maintainability and adaptability.

\begin{figure}[htbp]
	\centerline{\includegraphics[width=0.99\linewidth]{figs/MOT_Framework_Integration_SPENT_SANT.png}}
	\caption{
A schematic representation shows the integration of two \acp{NN} (highlighted in dark blue) within a TbD framework.
The Association Network can be implemented using either \ac{SANT} or \ac{MANTa}, and works in tandem with the prediction network \ac{SPENT}, which predicts object states $X_{t,1:n}$ across time stamps based on tracked objects ${T^c}_{t,1:n}$, which are then associated with sensor observations $Z_{t,1:m}$ by \ac{SANT} or \ac{MANTa}.
}
	\label{pic::MotFramework}
\end{figure}

\section{Contributions and overview}
Our primary contribution is the development, single evaluation and joint integration of three novel \ac{NN} that we label:
\begin{enumerate}[(i)]
    \item SPENT (Single-Prediction Network) which predicts the states of tracked objects.
    \item SANT (Single-Association-Network) which associates one incoming sensor objects to all currently tracked objects.
    \item MANTa (Multi-Association Network) which associates multiple incoming sensor objects to all currently tracked objects.
\end{enumerate}

Figure {\ref{pic::MotFramework}} provides an overview of our approach in which the association network can be implemented using either \ac{SANT} or \ac{MANTa}.
The input for the proposed prediction network (i) are the \acp{SO} $Z_{t,1:m}$ in every time stamp.
If new objects are detected by the sensors, these are stored in a $k$-dimensional state vector containing information such as object position $(x,y)$, yaw angle and object dimensions
(length and width) (for $k=5$).
\ac{SPENT} predicts all vectors $X_{t,1:n}$ to the next time stamp where they are used as input to either the SANT or MANTa association networks.
These provide the association matrix $A_{t,1:m}$, that is used to update the tracks ${T^u}_{t,1:n}$ using the corresponding sensor objects or create new tracked objects.
The Track Management can then decide to delete tracks, that were not updated for a specific amount of time and send out tracks ${T^c}_{t,1:n}$ to the next higher software component that have been confirmed by sensor objects.
(i) In contrast to the \ac{KF} {\cite{KF_BoundingBD.2023}}, our proposed prediction network is capable of predicting the state of individual objects without the need for a predefined heuristic prediction model. %MD: Bitte Formulierung prüfen
The self-learning, data-driven approach enables adaptability to various scenarios and the ability to effectively handle non-linearities and habits of road users.
Many conventional tracking systems rely on static methods for data association.
Commonly used algorithms like the \ac{HA} {\cite{DA_hungarian.1955}} require heuristics and fixed thresholds.
(ii) Our proposed \ac{SANT} and (iii) \ac{MANTa} replace the calculation of a distance metric for the corresponding assignment by employing \ac{ML} in order to resolve situations unclear for traditional approaches.
Both, our prediction network and our association networks can be developed and evaluated as stand-alone models.
For a throughout evaluation, we proceed by integrating them into an existing tracking system and demonstrate their performance through multiple tests and comparisons with established methods.
This work provides new insights and advancement in the development of \ac{ADAS} tracking systems by applying \ac{ML}\@.

\section{Related work}
\subsection{State prediction for tracked objects}
One fundamental problem in \ac{TbD} frameworks is the prediction of the states of the already tracked objects.
In many approaches Kalman filters and their variants have proven to be effective for state prediction {\cite{KF_simple_cues.2022, KF_Bewley.2016, KF_framework.2013}}.
However, they reach their limits in more complex scenarios, particularly in the presence of non-linear motion patterns and interactions among multiple objects {\cite{KF_Ristic.2004,KF_Julier.2004,KF_Wan.2000}}.
Ristic et al. {\cite{KF_Ristic.2004}} highlight the limitations of Kalman Filters in handling nonlinear and non-Gaussian cases, introducing Particle Filters as a potential alternative.
Julier et al. {\cite{KF_Julier.2004}} extend the standard Kalman Filter with the Unscented Kalman Filter (UKF) to better address nonlinear motion models.
Wan et al. {\cite{KF_Wan.2000}} propose the use of Gaussian Mixture Models for tracking multiple objects in cluttered environments.
In this work, we introduce a novel \ac{MOT} approach that leverages \ac{ML} to overcome these challenges.
We specifically focus on the development and implementation of \acp{NN}, which can enable more precise and flexible data-driven object state predictions.

\subsection{Association of sensor objects to tracks}
Another key challenge for TbD trackers is data association.
The widely used \ac{GNN} algorithm, often implemented via the \ac{HA} {\cite{DA_hungarian.1955}}, assigns detections to tracks by minimizing a distance metric.
However, it only considers current observations, ignoring temporal continuity and motion patterns, which can lead to errors in complex scenarios such as crossing objects or noisy sensor data.
While methods like the \ac{JPDA} {\cite{DA_JPDA.1993}} evaluate the likelihood of all possible assignments, they are computationally expensive.

\ac{ML}-based approaches have been proposed to address these limitations.
The temporal information in tracks can for example be leveraged through the attention mechanisms as used in {\cite{DL_ATT_CNN_soda.2020, DL_ATT_CNN_mot_sot_based.2017}} or \acp{RNN} as developed in {\cite{DL_RNN_mot.2016, DL_RNN_data_association.2019}}.
The latter is what we are also pursuing in this work.
Similar to the problem statement by Mertz et al. {\cite{DL_RNN_data_association.2019}}, the aim of our work is to develop a data-based approach that can learn to completely solve the combinatorial non deterministic polynomial time (NP) hard optimization problem of data association.
In the context of this work, we put forward the hypothesis that a \ac{GRU}-based association network can be designed and trained using an undefined distance measure, as discussed in the next chapter.

\subsection{Real time applications}
Tracking systems for ADAS run on embedded devices in real-time. They must provide  immediate state prediction of objects directly after their first detection {\cite{KF_Bewley.2016,DL_Wojke.2017}}, making offline tracking methods like {\cite{offline_mot.2017}} unsuitable. Consequently, modern multi-object tracking approaches rely on online methods, which do not have access to future sensor data. These methods estimate object-track similarity based on predicted positions or object features like appearance.

\paragraph{Kalman Filter based tracker}
Our \ac{ML} models are integrated into a \ac{KF}-based tracking system, widely used for its robust performance and interpretability {\cite{KF_Wan.2000,KF_Ristic.2004,KF_Julier.2004,KF_framework.2013,KF_Bewley.2016,KF_simple_cues.2022,KF_BoundingBD.2023}}. Bewley et al. {\cite{KF_Bewley.2016}} introduced an efficient multi-object tracking method combining a KF with the Hungarian Algorithm. Seidenschwarz et al. {\cite{KF_simple_cues.2022}} proposed a simpler approach based on visual cues like color, shape, and motion for object tracking and frame-to-frame association, avoiding the complexity of many modern trackers.

\paragraph{Recurrent Neural Network based Tracker}
Similar to our methodologies, RNN-based approaches for online multi-target tracking have been introduced in~{\cite{DL_RNN_mot.2016,DL_RNN_data_association.2019,DL_RNN_data_association.2020}}.
Specifically, the work by Mertz et al. {\cite{DL_RNN_data_association.2019}} focuses on data association within a TbD framework.
Their proposed DeepDA model, a Long Short-Term Memory (LSTM)-based Deep Data Association Network, is designed to learn and execute the task of associating objects across frames.
This model's ability to discern association patterns directly from data enables a robust and reliable tracking outcome, even in environments with significant disturbances.
Mertz et al. employ a distance matrix, derived from the euclidean distance measure, as the input for the DeepDA network.
This innovative approach effectively supersedes traditional association algorithms, such as the Hungarian Algorithm.
It is inferred that the euclidean distance measure served as a foundation not only for generating the ground truth (GT) training data (i.e., distance matrices) but also for the subsequent evaluation process.
However, this is not explicitly stated.
In our work, we want to enable the network to follow a completely data-driven association logic without forcing a concrete distance metric.

\paragraph{Attention Mechanism based Tracker}
In this paper, we analyze tracks using \ac{LSTM}-based models.
An alternative architecture would be the attention mechanism {\cite{ML_Attention.2017}}, utilized in various studies {\cite{DL_ATT_mot_sot_based.2017,DL_ATT_CNN_mot_sot_based.2017,DL_ATT_CNN_soda.2020,DL_CNN_ATT_mot.2017,DL_ATT_Trackformer.2022}}.
Hung et al. {\cite{DL_ATT_CNN_soda.2020}} focus on soft data association (SoDA), enabling probabilistic associations and accounting for uncertainties by aggregating information from all detections within a temporal window.
This approach allows the model to learn long-term, interactive relationships from large datasets without complex heuristics.
However, since tracking tasks involve real-time data processing with relatively short sequences, \acp{RNN} can efficiently handle this without the overhead of calculating attention weights for every input, making them more computationally efficient for short to medium-length sequences.

\section{Tracking with \ac{ML}-based Prediction and Association Networks}
We apply the \ac{TbD} paradigm, in which a tracker fuses object detections to generate object tracks that are consistent over time. In our study a \ac{KF} framework was implemented following the computational ideas of Vo et al. {\cite{KF_framework.2013}}.
To enable our models to incorporate temporal information, we use LSTM {\cite{DL_LSTM.1997}} and bidirectional Long Short-Term Memory (BILSTM) network layers {\cite{DL_BILSTM.1997}}, both for the prediction and the association of the sensor objects to the existing tracks.

\subsection{Single Prediction Network (SPENT)}
\label{ssec:spent}
Our approach uses the hidden states of the \ac{LSTM} layer as an information repository for each object. {\ac{SPENT}} operates in an open-loop manner, predicting future states based on past data.
This allows the network to predict the most likely state of an object for the next timestamp.

\paragraph{Data preprocessing}
In the development of our model, \ac{GT} data comprising vehicle tracks (cars and vans) from the KITTI dataset {\cite{Geiger2012CVPR}} was utilized.
We extracted 635 tracks, filtering out shorter tracks using a 3-frame threshold, resulting in 624 tracks for training.
This ensures the network receives tracks with sufficient time stamps, a common practice in tracking systems \cite{DL_DetTrack.2024}.
The track lengths vary from a minimum of 4 frames to a maximum of 643 frames (Sequence 20, ID 12).

To enhance model generalization and foster convergence during training, we normalized the state values of tracks at time $t$ (predictors) and at time $t+1$ (targets) in accordance with the methodology outlined in {\cite{DL_book.2019}}.
This normalization process standardizes the distribution of both predictors and targets to have a mean of zero and a unit variance.
The mean values $\vec{\mu}$ and standard deviations $\vec{\sigma}$ for each state in the state vectors $\vec{X}_t$ were computed across all tracks. For this, all tracks were joined together, leading to one pseudo-track with a total number of time stamps $N$:

\begin{equation}
	\vec{\mu} = \frac{1}{N} \sum_{t = 1}^{N} \vec{X}_t
\quad\text{and}\quad
	\vec{\sigma} = \sqrt{{\frac{1}{N - 1}} \sum_{t = 1}^{N} (\vec{X}_t - \vec{\mu})^{2^*}},
\end{equation}

where for $\vec{\sigma}$, the square $(...)^{2^*}$ and the square-root must be applied element wise.

\begin{figure}[b!]
	\centerline{\includegraphics[width=0.91\linewidth]{figs/padding2.png}}
    \caption{Analysis of sequence padding: unsorted vs. sorted data. This figure illustrates the impact of sequence padding on LSTM training based on the sorting of input data. The upper panel shows that unsorted data requires extensive padding to equalize batch sequence lengths, increasing computational overhead. In contrast, the lower panel demonstrates that sorting data by length before batching significantly reduces the necessary padding.}
	\label{pic::Padding}
\end{figure}


In our approach, we apply pre-padding as described by Reddy et al. {\cite{DL_padding.2019}}, who examined the impact of padding strategies on sequence-based \acp{NN}.
They highlight that while both pre-padding and post-padding are feasible, the choice significantly affects performance, especially for \ac{LSTM}-based networks, where maintaining sequence context is critical.
To manage varying sequence lengths, we pad shorter sequences with special tokens, ensuring all sequences in a batch have uniform length.


We used zeros as padding tokens, added to the end of sequences as needed, allowing efficient batch processing and consistent training without affecting model performance due to varying sequence lengths.
As noted by Reddy et al. {\cite{DL_padding.2019}}, while padding introduces noise, it is crucial for aligning sequences in mini-batches for \ac{LSTM} training.
To reduce this noise, we sorted the training dataset by sequence length before applying mini-batch padding.
This method, illustrated in figure {\ref{pic::Padding}}, significantly minimizes the padding (shown in turquoise) required for each mini-batch, which was essential for achieving convergence during training.

\paragraph{Network architecture}
As depicted in figure \ref{pic::SpentArch}, the schematic illustration of the generic structure of the prediction network illustrates how the architecture is adeptly designed to address the challenges of real-time state prediction.
This representation highlights the strategic deployment of the \ac{LSTM} layer for storing and processing object-specific information, facilitating accurate and timely predictions of object states.

\begin{figure}[bp]
	\centerline{\includegraphics[width=0.95\linewidth]{figs/SPENT_network_arch.png}}
	\caption{Schematic representation of the generic structure of \ac{SPENT}.}
	\label{pic::SpentArch}
\end{figure}

The foundational layer of our \ac{SPENT} is an \ac{LSTM} layer, where hidden states are dynamically updated at each time stamp based on incoming measurement data.
This mechanism allows continuous correction throughout each track's sequence, enhancing predictive accuracy.
The number of hidden units correlates with the amount of information retained over time, as shown in figure \ref{pic::SpentArch}.
These hidden states encapsulate information from all preceding time stamps, ensuring comprehensive temporal understanding.
Following the \ac{LSTM} layer, we incorporate a Batch-Normalization layer which accelerates training and promotes convergence by mitigating internal covariate shift \cite{DL_batch_norm.2015}.
Next ist a Relu layer, which applies a non-linear threshold operation, setting values below zero to zero.
During training, a Dropout layer randomly nullifies input elements with a specified probability, regularizing the model and preventing overfitting \cite{DL_dropout.2014}.
The architecture concludes with a Fully Connected (FC) Layer, which integrates insights from previous layers, with its dimensionality aligned to the number of required output variables {\cite{DL_FC.2010}}.

Our model’s loss function is based on the Mean Squared Error (MSE) metric, which is calculated for each state value prediction.
The MSE quantifies the average squared discrepancy between the predicted and actual target values.
We chose MSE because it provides a clear and direct measure of how closely our predictions align with the true states, making it an effective metric for optimizing the accuracy of our model's state predictions.

For one single prediction the \ac{MSE} is given by
\begin{equation}
	MSE = \frac{1}{k}\sum_{i=1}^{k}{({Y}_i-{\hat{Y}}_i)}^2,
\end{equation}
where $k$ is the length of the predicted state vector (here $k=5$), $Y_i$ are the entries of the ground truth state vector (KITTI cars and vans tracks) and $\hat{Y}_i$ the respective entries of the predicted state vector from our network.
During training, the cost function is evaluated for one mini-batch with several sequences and a total number of $N$ time stamps.
It is calculated as half the mean-square-error of the predictions added up for each time stamp, normalized over all time stamps.
The factor of $\frac{1}{2}$ simplifies the gradient during backpropagation:
\begin{equation}
	cost = \frac{1}{2}\frac{1}{N} \sum_{t=1}^{N}\frac{1}{k}\sum_{i=1}^{k}{({Y_{t,i}}-{\hat{Y}_{t,i}})}^2,
\end{equation}
where $Y_{t,i}$ refers to the $i$-th entry in the state vector at time stamp $t$.

\subsection{Single Association Network (SANT)}
Our association network uses a data-driven approach to solve the NP-hard data association problem {\cite{DA_hungarian.1955,DA_JPDA.1993}}, which traditionally requires significant computational effort for optimal solutions {\cite{DA_hungarian.1955}}.
Unlike conventional methods {\cite{DL_RNN_mot.2016, DL_RNN_data_association.2019}}, our \ac{SANT} model eliminates the need for a predefined distance matrix. Instead, it directly processes the current tracks and newly detected sensor objects, matching each new object to a track. This allows the network to autonomously learn its association strategy from training data, replacing the Hungarian Algorithm with a learning-based method.

\paragraph{Data preprocessing}
In the formulation of \ac{SANT}, we conceptualized data association as a temporally structured challenge, adopting a sequence-to-vector paradigm. In general, $m$ incoming sensor objects need to be associated to $n$ tracks.
For \ac{SANT} we focus on the association of a singular sensor object ($m=1$), denoted as $Z_{(t,m=1)}$, to $n$ tracks, represented as $X_{(t,1:n)}$.
These tracks were carefully curated from the KITTI dataset, which comprises annotated camera recordings.
%MD: Ich habe die Erklärung umgeschrieben, damit jemand anderes versteht was wir tun.
We note, however, that genuine hand-labelled \ac{GT} data for this specific association problem is not available. We rather use the existing tracks to  generate synthetical \ac{GT} data for the data association. The input to \ac{SANT} consists of a sensor object $Z_{(t,m=1)}$ and a set of tracks $X_{(t,1:n)}$ (see top part of figure \ref{pic::SantArch}). The output is a one-hot vector encoding the association of the incoming \ac{SO} to one of the tracks, or to none. To generate the synthetical \ac{GT}, we take a set of tracks, then randomly chose one of them and take the next state vector at the next time stamp of that single track as the incoming new \ac{SO}. The association result can be extracted from the corresponding track that it was taken out of.
Furthermore, to simulate more realistic sensor data, artificial noise was introduced.
This process was designed to reflect the inherent inaccuracies and uncertainties present in real-world sensor measurements.
To achieve this, we add noise to the state vector of the incoming \ac{SO}s with a maximum of 3\%.
% The data set was created in 6 iterations, so that the noise intensity was increased by 0.5\% per iteration.
As shown in figure {\ref{pic::SantArch}} the data format was created accordingly to enable index-based track assignment for \ac{SANT}. The actual number of tracks can vary between 0 and a maximum of $n=16$ objects, as is given by the KITTI data set using our selected objects (cars and vans) {\cite{Geiger2012CVPR}}. The size of the input matrix therefore corresponds to $k \times (n + 2)$, where $k = 5$ is the number of state values for our work.
%CH: Erklärung für +2 hinzugefügt.
%MD: Das verstehe ich nicht. Was soll denn eine Änderung des Inputs dafür bewirken? Was steht denn in dem input n + 2 drin? Wahrscheinlich nichts. Das Netzwerk würde doch auch keine neuen Tracks erstellen, sondern wenn dann ausgeben, dass das SO nicht assoziiert wurde. Hinzu kommt, dass du in dem ersten Satz hier zwei mal direkt das gleiche sagst "set up new tracks" kommt zwei mal vor... So etwas meine ich mit "veröffentlichungsfertig".
To give the network the freedom to set up new tracks and also the option to not associate or set up a new track, the size $n + 2$ must be defined.

A deeper analysis of our data is presented in the \ac{MANTa} section.

\paragraph{Network architecture}
The network as depicted in figure {\ref{pic::SantArch}} is designed as a sequence-to-classification network.
At each time stamp, a matrix is passed as input holding both, the information of the one to-be-assigned \ac{SO} and the multiple currently-tracked objects.

\begin{figure}[b!]
	\centerline{\includegraphics[width=0.95\linewidth]{figs/SANT_network_arch.png}}
	\caption{Schematic representation of the network structure of \ac{SANT}. Here $m=1$, so one \ac{SO} is associated to $m$ existing tracks.}
	\label{pic::SantArch}
\end{figure}
Architectures with \ac{RNN}, \ac{GRU}, \ac{LSTM} and \ac{BILSTM} layers were tested.
The best association performance was achieved with a \ac{BILSTM} layer.
The best performing architecture is depicted in figure {\ref{pic::SantArch}}. It achieved a validation accuracy of 95\%. %MD: Nirgendwo schreiben wir, wie groß die trainings und validation sets waren für die Association. Das machst du unten nur für die Prediction. Sind die gleich? Irdendwo müssen wir das noch schreiben. Und: Haben wir auch eine test accuracy für die Association?
We are using the work introduced by Hochreiter et al. {\cite{DL_LSTM.1997}}, who demonstrated the ability to capture the context from both ends of the sequence by combining the outputs of two LSTM layers that pass the information in opposite directions.
The resulting architecture is called \ac{BILSTM}.
The output mode has been configured in the \ac{BILSTM} layer, so that the layer is able to receive a sequence as input and calculate an output vector.
This form of dimension reduction is necessary in order to carry the classification.
The final FC layer specifies the number of classes via the number of output values.
The class probabilities are calculated in the softmax layer by applying the softmax function.
The cross-entropy cost function is utilized to quantify the discrepancy between the network's probabilistic predictions and the ground truth values, a method particularly suited for tasks involving categorically exclusive classes.
This approach employs one-hot encoding to transform class representations into binary vector formats.

We calculate the cost function as the average of the cross-entropy losses for each prediction, relative to its corresponding target value:
\begin{equation}
    cost = -\frac{1}{N} \sum_{i=1}^{N} \sum_{j=1}^{C} y_{i,j} \log(\hat{y}_{i,j}),
\end{equation}
where $N$ denotes the total number of association samples, $C$ represents the number of class categories, $y_{i,j}\in [0,1]$ is the GT indicator for whether class $j$ is the correct classification for sample $i$, and $\hat{y}_{i,j}$ is the predicted probability that sample $i$ belongs to class $j$, as derived from the softmax function output. Note that we are using non-capital $y$ to separate the notation from the state vectors $\vec{Y_{t}}$.

\subsection{Multi-Association Network (MANTa)}
The development of the \ac{SANT} demonstrated that data-driven association logic can be effectively learned by a deep learning model.
Building upon this foundation, we developed MANTa with the objective to create a network capable of associating multiple sensor objects ($m$ SOs) with multiple tracks ($n$ tracks) in a single operational step.
With MANTa, the following association scenarios can be addressed:

%CH: es könnte auch dargestellt werden, was mit MANTa adressiert wurde (m to x)
\begin{itemize}
    \item \textbf{1 to n} - one sensor object (SO) to $n$ tracks %(as \ac{SANT})
    \item \textbf{m to 1} - $m$ sensor objects (SOs) to one track
    \item \textbf{m to n} - $m$ sensor objects (SOs) to $n$ tracks
    \item \textbf{m to 0} - $m$ sensor objects (SOs) to no tracks
\end{itemize}
    % \item \textbf{0 to n} - no sensor objects (SOs) with $n$ tracks %(as \ac{SANT})
    % \item \textbf{0 to 0} - no sensor objects (SOs) with no tracks %(as \ac{SANT})
% \end{itemize}

%CH: habe ich jetzt komplett rausgenommen
% Being able to cover these scenarios enables \ac{MANTa} to adeptly manage the complexities inherent in \ac{MOT}, providing a robust and scalable solution for real-world applications.
% With the integration of \ac{MANTa} into a \ac{MOT} framework, the question can be asked whether a 0 to n and 0 to 0 assignment is a task to be solved.
% If no new \ac{SO} is detected, the prediction of the last operation step can be continued until the track is deleted on the basis of the decreasing probability of existence within the track management module.
% The association algorithm or the \ac{MANTa} does not need to be called if no tracks and \acp{SO} are detected.
% Although these assignment options can therefore be resolved via the programme structure in the \ac{MOT} framework, these options are also taken into account.
% This is intended to ensure that the network also learns to deal with \acp{SO} and tracks that are no longer available and capture information over time when no data is available.

\paragraph{Data preprocessing}
The training, validation and test data set of \ac{MANTa} was created according to the described objective.
Figure {\ref{pic::MantaData}} shows the input data structure with corresponding association tasks for time stamp 85 of sequence 20 of the KITTI data set. Equivalent data points were extracted across all sequences.
All extracted tracks were each modified with noise as described in section \ref{ssec:spent} and then normalized.
Figure {\ref{pic::MantaData}} shows an assignment example with non-noisy \acp{SO}s. The association result is given by the one-hot vector at the bottom. Each sensor object is given a 1:18 one-hot vector as its association result. For a maximum of $T_{\max}=16$ tracks, this results in an output vector of size $16\cdot 18 = 288$. The ordering of the \ac{SO}s is randomized per time stamp and the resulting association input matrix has dimension $F_{total} \cdot T_{\max}$, where $F_{total}$ represents the total number of features (here $2\cdot k=10$, with $k$ being the number of values per state vector). The one-hot vector shown in figure {\ref{pic::MantaData}} shows the GT assignment of the first sensor object to the track at position two.

%MD: Hier war einfach Text im PDF verdeckt, wenn du strip benutzt. Bitte schau ins PDF, das sehen auch die Leser nachher.
\begin{figure*}
	\includegraphics[width=\linewidth]{figs/MANTa_data.png}
	\captionof{figure}{MANTa, data structure, shows the non-noisy \acp{SO} to enable a visual assignment and increase understanding of the association procedure.
	Seven tracks are extracted from the KITTI data set for the given time stamp of sequence 20.
	Eight sensor objects are generated in pseudo-random order.
	The one-hot vector shows the GT assignment of the first sensor object to the track at position two.}
	\label{pic::MantaData}
\end{figure*}

%MD: Bis hier hin habe ich gelesen , hier stand noch m anstatt k =================================
There are seven tracks in the time stamp of the sequence shown.
Each track receives a new measurement in this time stamp, and an additional object was detected (new \ac{SO}), leads to a amount of 8 sensor objects.

The \ac{GT} assignment is displayed at the bottom of the section.
The assignment output can be thought of a matrix with dimensions maximum number of tracks $T_{\max}=16$ and the number of possible assignment classes $C=18$.
Where each field can either be zero (no assignment) or one (assignment).
For numerical reasons, we unfold this matrix to a one-hot vector of dimension $O_{\max}=288=16 \cdot 18$.
The assignment classes result from the described index class 1 to 16 and additional degrees of freedom.
One degree of freedom of the assignment represents the case that no measurement exists, another that the measurement should not be assigned.
The section of the one-hot vector shown (1:18) thus shows the GT assignment of the first sensor object to the track at position two.

As for SANT, the cross-entropy cost function calculates the cross-entropy loss between network predictions $\hat{y}_{i,j}$ and target values $y_{i.j}$ for the unique assignment task for mutually exclusive classes.
The already introduced one-hot vector is used to represent the class in binary form in a vector and thus generate a 1-to-$O_{\max}$ code.
The following formula is used to calculate the cross-entropy loss values for each time stamp $t$:
\begin{equation}
    loss_t = -\sum_{i=1}^{O_{\max}} \left[ y_{i,j} \ln(\hat{y}_{i,j}) + (1 - y_{i,j}) \ln(1 - \hat{y}_{i,j}) \right]
\end{equation}
Then, all scalars obtained per time stamp are summarized and divided by the number of samples $N$ of a minibatch for the cost function:
\begin{equation}
    cost = \frac{1}{N} \sum_{t=1}^{N} loss_t
\end{equation}

\paragraph{Network architecture}
The schematic representation {\ref{pic::MantaArch}} shows the developed network architecture for the simultaneous association of a large number of sensor objects to a large number of tracks.
This is what we call a \ac{MANTa}.

\begin{figure}[h]
	\centerline{\includegraphics[width=0.95\linewidth]{figs/MANTa_network_arch.png}}
	\caption{Schematic representation of the generic network structure of MANTa.}
	\label{pic::MantaArch}
\end{figure}

The \ac{BILSTM} layer processes the input data as already explained for \ac{SANT}\@.
The task of associating a \ac{SO} list with a track list requires a separate network part for each track.
This extension is labelled accordingly in figure {\ref{pic::MantaArch}}.
For each track (from 1 to $T_{\max}=16$), the MANTa has been developed with the fully connected, softmax stack that was introduced for \ac{SANT}.
Each softmax output consists of a vector with $C=18$ elements, which represents the most probable assignment.
This means that a single assignment can be realized for each track.
The vectors 1 to $T_{\max}$ are linked together in the Concatenation Layer.
This creates a vector with 288 elements, whereby 18 elements each represent the most probable assignment of a measurement to a track.

\section{Experimental Evaluation}
The developed networks were modularly integrated into the Tracking-by-Detection framework, replacing classical heuristic algorithms.
\ac{SPENT} substitutes the state predictions of the Kalman Filter, estimating predictions per time stamp without requiring a dynamics model.
Our recurrent network updates its internal hidden states at each time stamp, enabling accurate state predictions without external correction.
The model, suited for real-time applications, offers a strong alternative to traditional methods.
Using the KITTI dataset (cars and vans), our model achieved a \ac{RMSE} of 0.029 for positional predictions across 31 tracks (Table {\ref{tab::SpentRmseComparison}}), with average deviations of 0.42 meters on the X-axis and 0.23 meters on the Y-axis.
Using an inhouse implemented \ac{KF} carried by Research Group following {\cite{KF_BoundingBD.2023,KF_Bewley.2016}}, an RMSE of 0.066 was achieved on the same data set.

\begin{table}[htbp]
    \centering
    \caption{Comparison of \ac{RMSE} values for the \ac{SPENT} and \ac{KF} implemented by Research Group.}
    \resizebox{0.48\textwidth}{!}{%
    \begin{tabular}{|l|c|c|c|}
        \hline
        \textbf{-- Dataset --} & \textbf{Training} & \textbf{Validation} & \textbf{Testing} \\
        Number of tracks & 562 & 31 & 31 \\
        \hline
        \hline
        \textbf{Model} & \multicolumn{3}{c|}{\textbf{RMSE}} \\
        \hline
        SPENT & 0.025 & 0.027 & 0.029 \\
        \hline
        KF & 0.066 & 0.065 & 0.066 \\
        \hline
    \end{tabular}
    }
    \label{tab::SpentRmseComparison}
\end{table}

For another primary task of the \ac{TbD} \ac{MOT} method, data association, \ac{SANT} was developed as a replacement for the classical \ac{GNN} method.
\ac{SANT} can replace algorithms for calculating a distance metric and assignment procedures such as the Hungarian Algorithm with a learned, data-driven assignment logic.
Based on a defined validation dataset with approximately 2700 samples, \ac{SANT} achieves an accuracy of 95\%.

\ac{MANTa} is an advancement of \ac{SANT} and addresses the limitation of individual assignment.
This network extension is capable of assigning a set of \acp{SO} ($m$) to a set of tracks ($n$).
This means \ac{MANTa} is trained to assign a list of \acp{SO} to a list of tracks in a single operational step on the same dataset as \ac{SANT} and achieved an assignment accuracy of 95\% for the six most frequently occurring association sets.
Thus are sets with one to six tracks per time stamp.

\begin{table}[htbp]
    \centering
    \caption{Data association results for the SANT and MANTa networks.}
    \resizebox{0.48\textwidth}{!}{
    \begin{tabular}{|p{0.1\textwidth}|p{0.1\textwidth}|p{0.1\textwidth}|}
        \hline
        \textbf{--} & \multicolumn{2}{c|}{\textbf{Simultaneous Tracks per time stamp}} \\
        \textbf{ } & \textbf{1 to 6} & \textbf{7 to 16} \\
        \hline
        \hline
        \textbf{Model} & \multicolumn{2}{c|}{\textbf{Association Accuracy}} \\
        \hline
        \ac{SANT} & 95\% & 95\% \\
        \hline
        \ac{MANTa} & 95\% & 70\% \\
        \hline
    \end{tabular}
    }
    \label{tab::SpentRmseComparison}
\end{table}
In the context of the entire KITTI dataset, which includes both cars and vans objects,
MANTa achieves an average association accuracy of 70\%.
This limitation was primarily attributed to the characteristics of the extracted data.
As illustrated in figure {\ref{pic::MantaTracks}}, the distribution of the numberof existing tracks per time stamp reveals significant insights.

\begin{figure}[htbp]
    \centerline{\includegraphics[width=0.95\linewidth]{figs/MANTa_tracks.png}}
	\caption{The diagram shows the distribution of the number of existing tracks per time stamp.
		For the KITTI dataset, which includes both car and van objects. It reveals that time stamps containing one to six tracks constitute 81.5\% of the samples (6374 of 7827).
		The ledgend shows the absolute number of samples which contain the respective number of tracks.
		In 29.9\% of the samples, the data contains one track, in 14,7\% two tracks. Only 18.5\% of the samples contain more than six tracks, which leads to an unbalanced dataset (1448 of 7827 samples).}
	\label{pic::MantaTracks}
\end{figure}
Notably, time stamps containing at least one track constitute nearly one-third of the entire dataset, accounting for 29.9\% of the data, which corresponds to an absolute count of 2315 samples.
time stamps containing one to six tracks constitute 81.5\% of the samples and are therefore 6374 of 7827 samples.
Consequently, tests were conducted using a reduced dataset with one to six tracks per time stamp to demonstrate the multi-association capability of the network.
\ac{MANTa} correctly assigns 95\% of the dataset for time stamps containing one to six tracks.
This result confirms MANTa's proficiency in handling data with the appropriate dataset, as SANT also achieves a validation accuracy of approximately 95\% across the entire KITTI dataset (including cars and vans).
The primary advantage of \ac{MANTa} over \ac{SANT} is its ability to assign multiple sensor objects to multiple tracks in a single operational step.


\section{Conclusion}
In this work, we have developed and evaluated two neural network models, SPENT and SANT, as replacements for classical heuristic algorithms in the Tracking-by-Detection framework. SPENT effectively substitutes the state predictions of the Kalman Filter, achieving a \ac{RMSE} of 0.029 for positional predictions on the KITTI dataset, significantly outperforming the traditional Kalman Filter which achieved an \ac{RMSE} of 0.066.

SANT, designed for data association tasks, replaces the classical Global Nearest Neighbor method and achieves an impressive accuracy of 95\% on a validation dataset with approximately 2700 samples. Furthermore, MANTa, an extension of SANT, addresses the limitation of individual assignment by assigning sets of sensor objects to tracks in a single operational step, maintaining a high accuracy of 95\% for the most frequently occurring association sets.

Our results demonstrate that the developed models are suitable for real-time applications and represent viable alternatives to classical prediction and data association methods.
While the models show significant improvements over traditional methods, there are areas that require further investigation. The performance of MANTa on larger datasets and its ability to handle more complex association probabilities need to be explored.
Future research will focus on comparing the performance of attention mechanisms versus LSTMs in the context of state prediction and data association.
And also on evaluating the models on larger datasets to ensure scalability and robustness.

\section*{Acknowledgments}
We would like to express our sincere thanks to the organizations that provided financial support for this research project, namely the Daimler Truck AG and the Baden-Württemberg Cooperative State University (DHBW) Stuttgart.

% \menz{In some references, the author names are wrongly abbreviated.}

\bibliographystyle{IEEEtran}
\bibliography{IEEEabrv, lit.bib}

\vfill

\end{document}
