\documentclass[lettersize,journal]{IEEEtran}
\usepackage{amsmath,amsfonts}
\usepackage{algorithmic}
\usepackage{algorithm}
\usepackage{array}
\usepackage[caption=false,font=normalsize,labelfont=sf,textfont=sf]{subfig}
\usepackage{textcomp}
\usepackage{stfloats}
\usepackage{url}
\usepackage{verbatim}
\usepackage{graphicx}
\usepackage{cite}
\hyphenation{op-tical net-works semi-conduc-tor IEEE-Xplore}
% updated with editorial comments 8/9/2021


\begin{document}

\title{Multi-object tracking (MOT) without dynamic models and hard association metrics}

\author{Christian Alexander Holz, Christian Bader, Matthias Drüppel
	\thanks{C. Holz is with Daimler Truck AG, Research and Advanced Development, Stuttgart, Germany}
	\thanks{C. Bader is with Daimler Truck AG, Research and Advanced Development, Stuttgart, Germany}
	\thanks{M. Drüppel is with the Center for Artificial Intelligence,
		Duale Hochschule Baden-Württemberg (DHBW), Stuttgart, Germany}
}

% The paper headers
\markboth{Journal of \LaTeX\ Class Files,~Vol.~tbd, No.~tbd, tbd~2024}%
{Shell \MakeLowercase{\textit{et al.}}: A Sample Article Using IEEEtran.cls for IEEE Journals}

% \IEEEpubid{0000--0000/00\$00.00~\copyright~2021 IEEE}% Remember, if you use this you must call \IEEEpubidadjcol in the second% column for its text to clear the IEEEpubid mark.
\maketitle

% Unwritten rule: dont cite or reference figures or tables in the abstract
\begin{abstract}
    In this paper, we develop Machine Learning (ML)-based methods for Multi Object Tracking (MOT)
    within the context of Advanced Driver Assistance Systems (ADAS).
    Given the increasing complexity and demand for precise and efficient object tracking systems
    in the automotive industry, this work focuses on the integration of ML techniques into
    established tracking methodologies.
    Key contributions encompass the creation and evaluation of three specialized neural networks: (i)
    the Prediction Network for predicting the trajectories of tracked objects,
    (ii) the Single Association Network (SANT) for associating incoming sensor objects
    with existing tracks sequential, (iii) and the Multi Association Network (MANTa) for associating
    multiple sensor objects with existing tracks within one timestep.
    We combine our ML methods with a traditional Kalman filter framework,
    offering a data driven approach to address MOT challenges while maintaining the modularity and interpretability of classical filter approaches.
    We asses both, the performance of all three models alone as well as their impact on the performance when they are integrated in the Kalman framework.
    Based on the KITTI tracking data set Car, the Root Mean Square Error (RMSE) for predictions could be reduced with our Prediction Network by half compared to a basic Kalman Filter.
    Replacing single components allows us to get a clear evaluation of the impact of each ML model on the overall tracking system while maintaining the modularity of the system.
    The results reveal a modular, robust, and maintainable tracker,
    underscoring the potential of ML integration in AD Tracking Systems.
    % MD: hier müssen später noch unbedingt die wichtigsten konkreten Ergebnisse rein.
	% CH: Quantisierung der Performance: für Asso Network noch offen
	% HINTERGRUND: Idee ist Einleitung + Zusammenfasssung der wichtigsdten Ergebnisse
\end{abstract}

\begin{IEEEkeywords}
	Article submission, IEEE, IEEEtran, journal, \LaTeX, paper, template, typesetting.
\end{IEEEkeywords}

\section{Introduction}
%%%%%%%%%%%%%%%%%%%%%% Allgemeine Einführung MOT
\IEEEPARstart{T}{he}
ongoing evolution of Advanced Driver Assistance Systems (ADAS) has brought the need for precise
and reliable Multi Object Tracking (MOT) into the spotlight
\cite{KF_simple_cues.2022, KF_simple_online_realtime.2016}. In complex and dynamic environments, as encountered in urban traffic, it is crucial to
simultaneously and accurately capture the positions and movements of multiple objects - a key challenge in computer vision (CV).

In the commonly used Tracking-by-Detection (TbD) paradigm,
a tracker fuses detected sensor objects (SO) to create consistent object tracks over time.
A key challenge within this paradigm is associating the incoming measured SO
with their corresponding existing object tracks or initializing new object tracks.

Tracking frameworks form the heart of ADAS systems that are used in millions of vehicles around the globe. The vast majority of these frameworks rely on classical approaches such as the Kalman filter (KF) or its variants. These classical tracking theories have the great benefit of being modular and interpretable. The task is split into clearly separated subtasks such as the prediction of currently tracked objects and the association with newly measured ones. Furthermore the math and the theory itself is often clean and comprehendible. However, in the automotive industry these tracking systems need to be applied to a variety of different car models with different sensor sets, different installation heights of the sensors, different countries and always perform for a wide range of driving scenarios. This often leads to the increasing implementation of heuristics and parameters that tweak the tracking system for specific configurations and situations. But hand-engineered parameters and heuristics are hard to maintain and develop further from a software engineering point of view. Furthermore, performance of classical approaches can reach its limits in challenging situations.

In this work, we propose a data driven approach to tracking frameworks, which would allow the same system to be fine tuned for specific configurations relying only on data, thus increasing maintainability and adaptability. We do this preserving one of the biggest  strengths of classical approaches: its modularity, by replacing only single tracking components with ML models.

In comparison to the Kalman filter, our Prediction Network is capable of predicting the state of individual objects without the need for a predefined state or observation model at runtime.
Our Prediction Network holds the potential, particularly in terms of adaptability to various scenarios
and the ability to effectively handle nonlinearities.
Many conventional tracking systems rely on static methods for data association,
often based on simple heuristics or fixed thresholds.
In contrast, Single Association Network (SANT) employs machine learning to automate these processes and adapt more effectively to different scenarios.
As a result, within the Tracking-by-Detection (TbD) MOT framework,
SANT replaces the calculation of a distance metric and the Hungarian algorithm for
the corresponding assignment.
Furthermore, we integrate the Prediction Network and SANT into an existing tracking system and demonstrate
their performance through multiple tests and comparisons with established methods.

This work provides new insights and advancement in the
development of Advanced Driver Assistance Systems (ADAS),
contributing to the further evolution of technologies for autonomous driving (AD).

% MD: Vielleicht können wir noch ein paar Arbeiten finden die ML für tracking nutzen.
% MD: Wir sollten insgesamt auf 35++ zitierte Arbeiten kommen. Oft ist das Einfachste sich eine gute andere Arbeit aus möglichst dem Gleichen Themengebiet zu nehmen und zu schauen wen die gerade zitieren.
% CH: Paper aus der Thesis ergänzt. Zusäzliche Arbeiten müssen noch gesucht werden (aktuell ca. 15 Quellen angegeben)
\section{Related work}
\subsection{Tracked object prediction}
One fundamental problem in Tracking-by-Detection (TbD) frameworks is the prediction of the states of the already tracked objects. In many approaches Kalman filters and their variants have proven to be effective for state prediction\cite{KF_simple_cues.2022, KF_simple_online_realtime.2016, KF_framework.2013}. 
However, they reach their limits in more complex scenarios, particularly in the presence
of non-linear motion patterns and interactions among multiple objects. 
%MD a): finden wir hier auch ein Paper? Grenzen von Kalman filtern, sowas? %CH: ... 
% oder b): as similary motivated in cite / oder by name et al. cite 
In this work, we introduce a novel MOT approach that leverages Machine Learning (ML)
to overcome these challenges.
We specifically focus on the development and implementation of Neural Networks (NN),
which can enable more precise and flexible data-driven object tracking.

\subsection{Association}
Another fundamental problem for a TbD Tracker is the data association. 
For this some approaches only consider the current state of the tracks,
while others integrate temporal information such as the track history.
This aggregate of temporal information can be done for example using attention mechanisms as used in \cite{DL_ATT_CNN_soda.2020, DL_ATT_CNN_mot_sot_based.2017} or recurrent neural networks (RNNs) as in \cite{DL_RNN_mot.2016, DL_RNN_data_association.2019}. The latter is what we are also pursuing in this work.
Similar to the problem statement in Mertz et al \cite{DL_RNN_data_association.2019}, the aim of our work is to develop a data-based approach that can learn to completely solve the combinatorial Non Deterministic Polynomial Time (NP) hard optimization problem of data association.
Mertz et al \cite{DL_RNN_data_association.2019} use a distance matrix based on the
Euclidean distance measure as input for the developed association network,
thus replacing an association algorithm such as the Hungarian Algorithm (HA) \cite{DA_hungarian.1955}. 

In the context of this work, we put forward the hypothesis that a Gated
Recurrent Unit (GRU)-based association network can be designed and trained using an undefined distance measure. 
This can increase the assignment of the time-based memory units, i.e. the history, and thus boost performance. 
For this purpose, the Long Short-Term Memory network (LSTM) layer is implemented, which enables the memory of information using hidden units over a time interval \cite{DL_LSTM.1997}.
The goal of this work was therefore to develop an association network that is intended to solve
the assignment of one or more sensor objects (SO) to an existing number of object tracks
without a defined distance measure.

\subsection{Real time applications}
ADAS are embedded real-time applications where it is crucial to predict the state of objects immediately after their detection. This rules out offline tracking methods as presented in \cite{offline_mot.2017} that process the entire video material at once in a batch process.
Therefore, most recent approaches for tracking multiple objects rely on online methods
that do not depend on future image information. Online methods use various features to estimate the similarity between the recognized objects and the existing tracks.
This can be done on the basis of their predicted positions or even similarities in appearance.

\paragraph{Kalman Filter based Tracker}
\cite{KF_simple_cues.2022, KF_simple_online_realtime.2016, KF_framework.2013}
propose a Kalman Filter (KF) based TbD multi-object tracker.
\cite{KF_simple_cues.2022} presents a MOT approach based on simple visual cues. The authors contend that many existing multi-object trackers are too complex and require a large amount of computational resources. 
Instead, they propose a simpler approach based on basic visual features such as colour, shape and motion. 
These visual cues are used to track objects at the image level and make associations between frames.

\paragraph{Recurrent Neural Network (RNN) based Tracker}
How we also  
\cite{DL_RNN_mot.2016,
	DL_RNN_data_association.2019, 
	DL_RNN_data_association.2020}
present approaches for online multi-target tracking using recurrent neural networks (RNNs).
The approach presented in \cite{DL_RNN_data_association.2019} focusses on the task of data association in a TbD framework.
The developed DeepDA approach represents an LSTM-based Deep Data Association Network to learn and perform the association of objects between frames. 
By learning association patterns from the data, the tracker can achieve robust and reliable tracking results even in highly disturbed environments.
\cite{DL_RNN_data_association.2019} use a distance matrix based on the Euclidean distance measure as input data for the developed DeepDA network, thus replacing an association algorithm such as the Hungarian Algorithm (HA).
It can be assumed that the Euclidean distance measure was also used as the basis for generating the ground truth (GT) training data (distance matrices) and for the evaluation. 
However, this is not explicitly stated. 
It can therefore be argued that this preprocessing step deprives the network opportunity to follow a different association logic or to learn it data based.

\paragraph{Attention Mechanism based Tracker}
Each of the papers 
\cite{DL_ATT_mot_sot_based.2017,
	DL_ATT_CNN_mot_sot_based.2017,
	DL_ATT_CNN_soda.2020,
	DL_CNN_ATT_mot.2017, 
	DL_ATT_Trackformer.2022}
presents approaches for a tracker that utilise the attention mechanism \cite{ML_Attention.2017}, for example to compute soft data association \cite{DL_ATT_CNN_soda.2020}.
The main research focus of the paper \cite{DL_ATT_CNN_soda.2020} is on soft data association, which enables the tracker to make probabilistic associations between objects and account for uncertainties in the associations. 
Soft data association in the SoDA model works by using attention mechanisms to aggregate information from all detections in a given temporal window. 
This allows the model to learn long-term and highly interactive relationships between detections and tracks from large datasets without using complex heuristics and hyperparameters. 

\section{Overview of our proposed models}
Our primary contribution is the development and evaluation of three NN that we labeled:
(i) the Prediction Network, (ii) the Single Association Network (SANT),
and (iii) the Multi Association Network (MANTa).
Figure \ref{MOT_Framework_Integration_SPENT_SANT} provides an overview of our approach
in which the association network can be represented by (ii) or (iii) and the prediction network is represented by (i).
The input for the proposed Prediction Network (i) are the sensor objects (SO) in every time step. These objects consists of a fixed-dimensional state vector that contains information such as object position, orientation and dimension.
The Prediction Network predicts all vectors to the next time step where they are used as input to the Single Association Network (SANT)
to build target trajectories or object tracks.

\begin{figure}[htbp]
	\centerline{\includegraphics[width=1.0\linewidth]{figs/MOT_Framework_Integration_SPENT_SANT.png}}
	\caption{Schematic representation of the two integrated networks in a tracking-by-detection (TbD) framework.}
	\label{MOT_Framework_Integration_SPENT_SANT}
\end{figure}

\section{Tracking with Prediction and Association Networks}
We apply the tracking-by-detection (TbD) paradigm, in which a tracker fuses object
fuses the object detections to generate object tracks that are consistent over time.
\cite{KF_framework.2013}, for example, provides a framework for analyzing tracking approaches that follow the TbD paradigm. This was used accordingly in this study.

To enable our models to incorporate temporal information, we use Long Short-Term Memory (LSTM) and bidirectional Long Short-Term Memory (BiLSTM) networks. Both for the prediction and the association (SANT) of the sensor objects (SO) to the existing tracks.

\subsection{Prediction Network}
Most existing tracking methods associate incoming detections pairwise with object states predicted by a simple motion model, e.g. a constant velocity model, using a Kalman filter
\cite{KF_simple_cues.2022, KF_simple_online_realtime.2016, KF_framework.2013}.
However, recent work has demonstrated that aggregating temporal information as well as contextual information can improve the tracking of multiple objects by utilizing higher order information in addition to pairwise similarities between detections \cite{DL_RNN_mot.2016, DL_RNN_data_association.2019, DL_RNN_data_association.2020, DL_ATT_CNN_soda.2020}.
Our approach is to use the hidden states of the LSTM layer as an object-specific information storage. 
The Prediction Network was designed for an open-loop application, this means that the network predicts values for a future time step based on previously received data.
For use in an online MOT process, the network is therefore able to make a prediction about the most likely next state values using the state values received for a given Statevector of a Sensorobject (SO).

\begin{figure}[htbp]
	\centerline{\includegraphics[width=0.90\linewidth]{figs/SPENT_network_arch.png}}
	\caption{Schematic representation of the generic prediction network structure.}
	\label{SPENT_network_arch}
\end{figure}

\paragraph{Network architecture} 
The internal values (hidden states) of the LSTM layer are updated per time step based on the received measurement data.
This updating process therefore provides a correction over the course of the sequence of each track.
The number of hidden units (cells) corresponds to the amount of information that the layer remembers between time steps, as shown in Figure \ref{SPENT_network_arch}. 
The hidden states can contain information from all the previous time steps, regardless of the sequence length. 
The values of the hidden states of the LSTM layer are updated in each time step based on the received measurement data and the hidden states of the past timestep.
These updates therefore results in an internal correction over the course of the sequence.
%BATCHNORM
In our prediction network, we have placed a batch normalisation layer directly after the LSTM layer. This means that the output of the LSTM layer is normalised before being passed on to the subsequent Relu layer. This helped speed up training and improve convergence by reducing internal covariate shifts \cite{DL_batch_norm.2015}.
%RELU
A Relu layer performs a threshold operation to each element of the input, where any value less than zero is set to zero.
The Relu function, also known as Rectified Linear Unit, is defined as follows:
\begin{equation}
	\text{ReLU}(x) = \max(0, x)
\end{equation}
%DROPOUT
At training time, a dropout layer randomly sets input elements to zero with a given probability.
This operation effectively changes the underlying network architecture between iterations and helps prevent the network from overfitting \cite{DL_dropout.2014}.
%FC
The Fully Connected (FC) Layers multiplies the input by a weight matrix and then adds a bias vector. As shown in Figure \ref{SPENT_network_arch} all neurons in a FC layer connect to all the neurons in the previous layer. This layer combines all of the local information learned by the previous layers. 
The last FC layer must be equal to the number of response variables in the Regression Output layer \cite{DL_FC.2010}.
%REGRESSION OUT
The regression layer computes the half-mean-squared-error loss. 
On the output of the regression layer at the end of the network ("regressionoutput"), the network calculates the loss through the mean square error (MSE) for the prediction for each state value.
The MSE indicates the average of the squared difference between the model prediction and the target value, which we
use as the measure of the quality of the prediction. For a single observation, the MSE is given by:
\begin{equation}
	MSE = \sum_{i=1}^{R}\frac{{(gt_i-y_i)}^2}{R}
\end{equation}
where $R$ is the size of the predicted state vector, $gt_i$ is the ground truth value (Kitti car tracks) and $y_i$ is the prediction of the network for sample $i$.
For our sequence-to-sequence regression network the loss function of the regression layer is half the mean square error of the predictions for each time step, normalized by the sequence length $S$: 
\begin{equation}
	loss = \frac{1}{2S} \sum_{i=1}^{S}\sum_{j=1}^{R}{({gt_{ij}}-{y_{ij}})}^2.
\end{equation}
During training, the average loss is calculated using the observations in the mini-batch, so $S$ equals the mini-batch length. %MD: So you dont have a sequence length anymore. So nothing is add up if there are no prediction on certain time stamps.
% MD: More specific: "In relation", do you mean you calculated the RMSE with the same formular? Also: What does this mean? Is is good, is it bad? Will you talk about it later?
In relation to the KITTI data set (cars and vans), a Root Mean Square Error (RMSE) of 0.025 was achieved for the position prediction of all objects in the dataset.
Using a standard Kalman filter (KF), an RMSE of 0.066 was achieved on the same data set.


\paragraph{Data preprocessing}
Ground truth (GT) data in the form of tracks of vehicles from the KITTI data set was used for our development.
A GT Track contains the information of an object over time, starting with its appearance and ending with its exit from the sensor detection range.
In order to achieve better generalization and to increase the chance that the training converges, the track state values at time t (predictors) and track state values at time t+1 (targets) were normalized following
\cite{DL_book.2019}, such that the possible predictions and targets have a mean value of zero and a unit variance.
The mean value $\mu$ and the standard deviation $\sigma$ for each state variable were calculated for all tracks using the following equations:
\begin{equation}
	\mu = \frac{1}{N} \sum_{i = 1}^{N} A_i
\quad\text{and}\quad
	\sigma = \sqrt{{\frac{1}{N - 1}} \sum_{i = 1}^{N} |A_i - \mu|^2}
\end{equation}

where $A_i$ the combined length of all tracks and $N$ is the number of states. %MD: Or time 
%stamps? Even for me this is not exactly clear what is being normalized and how. %CH: added explanation
We also apply pre-padding as described in \cite{DL_padding.2019} where Reddy et al. show how padding influences the performance of neural networks in sequence-based tasks. 
Their study suggests that although both pre-padding and post-padding are feasible, the choice of padding technique can have a significant impact on the efficiency of the model, especially for LSTM networks where the sequence context is crucial. 
%MD: And what does this mean for us? You say it is crucial and then it is not described any further. How did you apply padding? &CH: added text and figure
As \cite{DL_padding.2019} described, padding can lead to noise in the network, but one sequence adjustment per mini-batch is required for training LSTM networks. 
To minimise the effect, the sequences of the training data were therefore sorted by length before the mini-batch padding. Figure \ref{figure_padding} clearly shows that the padding (turquoise area) is minimised by a sorted training data set per mini-batch. Without this modification, no convergence could be achieved in the training.
\begin{figure}[htbp]
	\centerline{\includegraphics[width=0.8\linewidth]{figs/padding.png}}
	\caption{Mini-batch padding effect, training data for LSTM networks.}
	\label{figure_padding}
\end{figure}

%%%%%%%%%%%%%%%%%%%%%% gedanklicher :) Absatztitel: SANT
\subsection{Single Association Network (SANT)}
SANT stellt einen datenbasierten Ansatz dar, welcher lernen kann, das kombinatorische Non Deterministic Polynomial Time (NP) hard Optimierungsproblem der Datenassoziation vollständig zu lösen.
Im Vergleich zu den meisten Assoziationsverfahren \cite{DL_RNN_mot.2016, DL_RNN_data_association.2019} stellt SANT einen Ansatz dar, welcher als Inputdaten keine definierte Distanzmatrix erhält, sondern die bestehenden Tracks sowie jede neue Messung (SO) erhält.
Damit entfällt die Definition eines Abstandsmaßes, womit dem Netzwerk die Freiheit gegeben wurde, einer eigenen Assoziationslogik zu folgen bzw. diese datenbasiert zu lernen.
Somit ersetzt SANT die Berechnung eines Abstandsmaßes sowie einen Assoziationsalgorithmus wie z.B. den Hungarian Algorithm (HA).

\begin{figure}[htbp]
	\centerline{\includegraphics[width=0.4\linewidth]{figs/SANT_network_arch.png}}
	\caption{Schematic representation of the generic network structure of SANT.}
	\label{SANT_network_arch}
\end{figure}

\paragraph{Datenvorverarbeitung}
Für die Entwicklung des Single Association Networks (SANT) wurde die Datenassoziation als zeitlich gegliedertes Problem (Sequenz-zu-Vektor) betrachtet.
Das Datenassoziationsproblem beschreibt somit die Zuordnung von einem SO zu einem Set von Tracks.
Die entsprechenden Tracks wurden aus dem KITTI Datensatz der gelabelten Kameraaufzeichnungen extrahiert.
Allerdings existieren für das betrachtete Problem der Assoziation keine realen GT Daten.
Um sicherzustellen, dass die Zuordnung eines Sensor Objekts (SOs) zu einem Track eines Tracksets genau eine korrekte Lösung besitzt, wurde das SO in einem Zeitschritt aus dem bestehenden Trackset eines Zeitschritts erzeugt.
Die Daten wurden entsprechend künstlich verrauscht, um aus den GT Daten realistische Sensordaten zu erzeugen.
Das Datenformat wurde entsprechend gewählt, um eine indexbasierte Trackzuordnung für das Single Association Network (SANT) zu ermöglichen.
Die Größe der Inputmatrix entspricht daher $m * n$. Mit $m = 2 * AnzahlZustandswerte$ und $n = Trackanzahl(t)$.

\paragraph{Netzwerkentwicklung}
Das Netzwerk (Fig. \ref{SANT_network_arch}) ist als Sequenz-to-Classification ausgelegt, wobei der sequenceinput die beschriebene Inputmatrix pro Zeitschritt darstellt.

BILSTM,...
Die vollständig verknüpfte Schicht (FC) gibt über die Anzahl der der Ausgabewerte die Klassenanzahl n vor. Die n Klassenwerte werden in der Softmax Schicht durch Anwendung der Softmaxfunktion
in eine eindeutige Wahrscheinlichkeitsverteilung berechnet (wie in Kapitel 2.2.1 beschrieben).
Durch Anwendung der Cross-Entropie-Operation in der Ausgabeschicht
(Classout) konnte SANT auf einen korrekten Zuordnungswert (GT
Klassenindex) trainiert werden.

Die Cross-Entropie-Kostenfunktion (Kreuzentropie) berechnet den Cross-Entropie-Verlust zwischen Netzvorhersagen und Zielwerten für die eindeutige Zuordnungsaufgabe (für sich gegenseitig ausschließende Klassen).
Dabei wird mittels One-Hot-Kodierung die Klasse binär in einem Vektor dargestellt und somit ein 1-zu-n Code generiert.
Nach folgender Formel werden die Crossentropie-Verlustwerte für jeden Eingabewert $Y_j$ und zugehörigen Zielwert (Targetvalue) $T_j$ elementweise berechnet:
\begin{equation}
loss_j = -(T_j ln Y_j + (1 - T_j) ln(1 - Y_j))
\end{equation}
Um einen Skalar $loss$ zu erhalten werden alle Verlustwerte $loss_j$ aufsummiert und durch die Anzahl der Samples $N$ geteilt. Optional können die Verlustwerte von definierten Samples mit dem Gewichtungsfaktor $w_j$ gewichtet werden:
\begin{equation}
loss = \frac{1}{N} \sum_{}loss_j w_j
\end{equation}
Eine entsprechende Nutzung des Gewichtungsfaktors $w_j$ kann bei Datensätzen mit unausgewogenen (imbalanced) Klassenverteilung hilfreich sein.



%%%%%%%%%%%%%%%%%%%%%% gedanklicher :) Absatztitel: MANTa
\subsection{Multi Association Network (MANTa)}

\paragraph{Datenvorverarbeitung}

\paragraph{Netzwerkentwicklung}

\section{Experimental Evaluation}
... KITTI-Car Benchmark.


\section{Conclusion}
The conclusion goes here.


\section*{Acknowledgments}
This should be a simple paragraph before the References to thank those individuals and institutions who have supported your work on this article.


{\appendix[Proof of the Zonklar Equations]
\section*{Proof of the First Zonklar Equation}
Appendix goes here.
\section*{Proof of the Second Zonklar Equation}
And here.
}

\bibliographystyle{IEEEtran}
\bibliography{IEEEabrv, lit.bib}

\vfill

\end{document}


