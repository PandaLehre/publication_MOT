\documentclass[lettersize,journal]{IEEEtran}
\usepackage{amsmath,amsfonts}
\usepackage{algorithmic}
\usepackage{algorithm}
\usepackage{array}
\usepackage[caption=false,font=normalsize,labelfont=sf,textfont=sf]{subfig}
\usepackage{textcomp}
\usepackage{stfloats}
\usepackage{url}
\usepackage{verbatim}
\usepackage{graphicx}
\usepackage{cite}

\usepackage{capt-of}  % <---
\usepackage{cuted}    % <===

\hyphenation{op-tical net-works semi-conduc-tor IEEE-Xplore}
% updated with editorial comments 8/9/2021
% --> 6 pages at all


\begin{document}

\title{Multi-object tracking without dynamic models and hard association metrics}

\author{Christian Alexander Holz, Christian Bader, Matthias Drüppel
	\thanks{C. Holz is with Daimler Truck AG, Research and Advanced Development, Stuttgart, Germany}
	\thanks{C. Bader is with Daimler Truck AG, Research and Advanced Development, Stuttgart, Germany}
	\thanks{M. Drüppel is with the Center for Artificial Intelligence,
		Duale Hochschule Baden-Württemberg (DHBW), Stuttgart, Germany}
}

% The paper headers
\markboth{Journal of \LaTeX\ Class Files,~Vol.~tbd, No.~tbd, tbd~2024}%
{Shell \MakeLowercase{\textit{et al.}}: A Sample Article Using IEEEtran.cls for IEEE Journals}

% \IEEEpubid{0000--0000/00\$00.00~\copyright~2021 IEEE}% Remember, if you use this you must call \IEEEpubidadjcol in the second% column for its text to clear the IEEEpubid mark.
\maketitle

% Unwritten rule: dont cite or reference figures or tables in the abstract
\begin{abstract}
    In this paper, we develop Machine Learning (ML)-based methods for Multi Object Tracking (MOT)
    within the context of Advanced Driver Assistance Systems (ADAS).
    Given the increasing complexity and demand for precise and efficient object tracking systems
    in the automotive industry, this work focuses on the integration of ML techniques into
    established tracking methodologies.
    Key contributions encompass the creation and evaluation of three specialized neural networks: (i)
    the Prediction Network for predicting the trajectories of tracked objects,
    (ii) the Single Association Network (SANT) for associating incoming sensor objects
    with existing tracks sequential, (iii) and the Multi Association Network (MANTa) for associating
    multiple sensor objects with existing tracks within one timestep.
    We combine our ML methods with a traditional Kalman filter framework,
    offering a data driven approach to address MOT challenges while maintaining the modularity and interpretability of classical filter approaches.
    We asses both, the performance of all three models alone as well as their impact on the performance when they are integrated in the Kalman framework.
    Based on the KITTI tracking data set Car, the Root Mean Square Error (RMSE) for predictions could be reduced with our Prediction Network by half compared to a basic Kalman Filter.
    Replacing single components allows us to get a clear evaluation of the impact of each ML model on the overall tracking system while maintaining the modularity of the system.
    The results reveal a modular, robust, and maintainable tracker,
    underscoring the potential of ML integration in AD Tracking Systems.
    % MD: hier müssen später noch unbedingt die wichtigsten konkreten Ergebnisse rein.
	% CH: Quantisierung der Performance: für Asso Network noch offen
	% HINTERGRUND: Idee ist Einleitung + Zusammenfasssung der wichtigsdten Ergebnisse
\end{abstract}

\begin{IEEEkeywords}
	Article submission, IEEE, IEEEtran, journal, \LaTeX, paper, template, typesetting.
\end{IEEEkeywords}

\section{Introduction}
%%%%%%%%%%%%%%%%%%%%%% Allgemeine Einführung MOT
\IEEEPARstart{T}{he}
ongoing evolution of Advanced Driver Assistance Systems (ADAS) has brought the need for precise
and reliable Multi Object Tracking (MOT) into the spotlight
\cite{KF_simple_cues.2022, KF_simple_online_realtime.2016}. In complex and dynamic environments, as encountered in urban traffic, it is crucial to
simultaneously and accurately capture the positions and movements of multiple objects - a key challenge in computer vision (CV).

In the commonly used Tracking-by-Detection (TbD) paradigm,
a tracker fuses detected sensor objects (SO) to create consistent object tracks over time.
A key challenge within this paradigm is associating the incoming measured SO
with their corresponding existing object tracks or initializing new object tracks.

Tracking frameworks form the heart of ADAS systems that are used in millions of vehicles around the globe. The vast majority of these frameworks rely on classical approaches such as the Kalman filter (KF) or its variants. These classical tracking theories have the great benefit of being modular and interpretable. The task is split into clearly separated subtasks such as the prediction of currently tracked objects and the association with newly measured ones. Furthermore the math and the theory itself is often clean and comprehendible. However, in the automotive industry these tracking systems need to be applied to a variety of different car models with different sensor sets, different installation heights of the sensors, different countries and always perform for a wide range of driving scenarios. This often leads to the increasing implementation of heuristics and parameters that tweak the tracking system for specific configurations and situations. But hand-engineered parameters and heuristics are hard to maintain and develop further from a software engineering point of view. Furthermore, performance of classical approaches can reach its limits in challenging situations.

In this work, we propose a data driven approach to tracking frameworks, which would allow the same system to be fine tuned for specific configurations relying only on data, thus increasing maintainability and adaptability. We do this preserving one of the biggest  strengths of classical approaches: its modularity, by replacing only single tracking components with ML models.

In comparison to the Kalman filter, our Prediction Network is capable of predicting the state of individual objects without the need for a predefined state or observation model at runtime.
Our Prediction Network holds the potential, particularly in terms of adaptability to various scenarios
and the ability to effectively handle nonlinearities.
Many conventional tracking systems rely on static methods for data association,
often based on simple heuristics or fixed thresholds.
In contrast, Single Association Network (SANT) employs machine learning to automate these processes and adapt more effectively to different scenarios.
As a result, within the Tracking-by-Detection (TbD) MOT framework,
SANT replaces the calculation of a distance metric and the Hungarian algorithm for
the corresponding assignment.
Furthermore, we integrate the Prediction Network and SANT into an existing tracking system and demonstrate
their performance through multiple tests and comparisons with established methods.

This work provides new insights and advancement in the
development of Advanced Driver Assistance Systems (ADAS),
contributing to the further evolution of technologies for autonomous driving (AD).

% MD: Vielleicht können wir noch ein paar Arbeiten finden die ML für tracking nutzen.
% MD: Wir sollten insgesamt auf 35++ zitierte Arbeiten kommen. Oft ist das Einfachste sich eine gute andere Arbeit aus möglichst dem Gleichen Themengebiet zu nehmen und zu schauen wen die gerade zitieren.
% CH: Paper aus der Thesis ergänzt. Zusäzliche Arbeiten müssen noch gesucht werden (aktuell ca. 15 Quellen angegeben)
\section{Related work}
\subsection{Tracked object prediction}
One fundamental problem in Tracking-by-Detection (TbD) frameworks is the prediction of the states of the already tracked objects. In many approaches Kalman filters and their variants have proven to be effective for state prediction\cite{KF_simple_cues.2022, KF_simple_online_realtime.2016, KF_framework.2013}. 
However, they reach their limits in more complex scenarios, particularly in the presence
of non-linear motion patterns and interactions among multiple objects. 
%MD a): finden wir hier auch ein Paper? Grenzen von Kalman filtern, sowas? %CH: ... 
% oder b): as similary motivated in cite / oder by name et al. cite 
In this work, we introduce a novel MOT approach that leverages Machine Learning (ML)
to overcome these challenges.
We specifically focus on the development and implementation of Neural Networks (NN),
which can enable more precise and flexible data-driven object tracking.

\subsection{Association}
Another fundamental problem for a TbD Tracker is the data association. 
For this some approaches only consider the current state of the tracks,
while others integrate temporal information such as the track history.
This aggregate of temporal information can be done for example using attention mechanisms as used in \cite{DL_ATT_CNN_soda.2020, DL_ATT_CNN_mot_sot_based.2017} or recurrent neural networks (RNNs) as in \cite{DL_RNN_mot.2016, DL_RNN_data_association.2019}. The latter is what we are also pursuing in this work.
Similar to the problem statement in Mertz et al \cite{DL_RNN_data_association.2019}, the aim of our work is to develop a data-based approach that can learn to completely solve the combinatorial Non Deterministic Polynomial Time (NP) hard optimization problem of data association.
Mertz et al \cite{DL_RNN_data_association.2019} use a distance matrix based on the
Euclidean distance measure as input for the developed association network,
thus replacing an association algorithm such as the Hungarian Algorithm (HA) \cite{DA_hungarian.1955}. 

In the context of this work, we put forward the hypothesis that a Gated
Recurrent Unit (GRU)-based association network can be designed and trained using an undefined distance measure. 
This can increase the assignment of the time-based memory units, i.e. the history, and thus boost performance. 
For this purpose, the Long Short-Term Memory network (LSTM) layer is implemented, which enables the memory of information using hidden units over a time interval \cite{DL_LSTM.1997}.
The goal of this work was therefore to develop an association network that is intended to solve
the assignment of one or more sensor objects (SO) to an existing number of object tracks
without a defined distance measure.

\subsection{Real time applications}
ADAS are embedded real-time applications where it is crucial to predict the state of objects immediately after their detection. This rules out offline tracking methods as presented in \cite{offline_mot.2017} that process the entire video material at once in a batch process.
Therefore, most recent approaches for tracking multiple objects rely on online methods
that do not depend on future image information. Online methods use various features to estimate the similarity between the recognized objects and the existing tracks.
This can be done on the basis of their predicted positions or even similarities in appearance.

\paragraph{Kalman Filter based Tracker}
\cite{KF_simple_cues.2022, KF_simple_online_realtime.2016, KF_framework.2013}
propose a Kalman Filter (KF) based TbD multi-object tracker.
\cite{KF_simple_cues.2022} presents a MOT approach based on simple visual cues. The authors contend that many existing multi-object trackers are too complex and require a large amount of computational resources. 
Instead, they propose a simpler approach based on basic visual features such as colour, shape and motion. 
These visual cues are used to track objects at the image level and make associations between frames.

\paragraph{Recurrent Neural Network (RNN) based Tracker}
How we also  
\cite{DL_RNN_mot.2016,
	DL_RNN_data_association.2019, 
	DL_RNN_data_association.2020}
present approaches for online multi-target tracking using recurrent neural networks (RNNs).
The approach presented in \cite{DL_RNN_data_association.2019} focusses on the task of data association in a TbD framework.
The developed DeepDA approach represents an LSTM-based Deep Data Association Network to learn and perform the association of objects between frames. 
By learning association patterns from the data, the tracker can achieve robust and reliable tracking results even in highly disturbed environments.
\cite{DL_RNN_data_association.2019} use a distance matrix based on the Euclidean distance measure as input data for the developed DeepDA network, thus replacing an association algorithm such as the Hungarian Algorithm (HA).
It can be assumed that the Euclidean distance measure was also used as the basis for generating the ground truth (GT) training data (distance matrices) and for the evaluation. 
However, this is not explicitly stated. 
It can therefore be argued that this preprocessing step deprives the network opportunity to follow a different association logic or to learn it data based.

\paragraph{Attention Mechanism based Tracker}
Each of the papers 
\cite{DL_ATT_mot_sot_based.2017,
	DL_ATT_CNN_mot_sot_based.2017,
	DL_ATT_CNN_soda.2020,
	DL_CNN_ATT_mot.2017, 
	DL_ATT_Trackformer.2022}
presents approaches for a tracker that utilise the attention mechanism \cite{ML_Attention.2017}, for example to compute soft data association \cite{DL_ATT_CNN_soda.2020}.
The main research focus of the paper \cite{DL_ATT_CNN_soda.2020} is on soft data association, which enables the tracker to make probabilistic associations between objects and account for uncertainties in the associations. 
Soft data association in the SoDA model works by using attention mechanisms to aggregate information from all detections in a given temporal window. 
This allows the model to learn long-term and highly interactive relationships between detections and tracks from large datasets without using complex heuristics and hyperparameters. 

\section{Overview of our proposed models}
Our primary contribution is the development and evaluation of three NN that we labeled:
(i) the Prediction Network, (ii) the Single Association Network (SANT),
and (iii) the Multi Association Network (MANTa).
Figure \ref{MOT_Framework_Integration_SPENT_SANT} provides an overview of our approach
in which the association network can be represented by (ii) or (iii) and the prediction network is represented by (i).
The input for the proposed Prediction Network (i) are the sensor objects (SO) in every time step. These objects consists of a fixed-dimensional state vector that contains information such as object position, orientation and dimension.
The Prediction Network predicts all vectors to the next time step where they are used as input to the Single Association Network (SANT)
to build target trajectories or object tracks.

\begin{figure}[htbp]
	\centerline{\includegraphics[width=1.0\linewidth]{figs/MOT_Framework_Integration_SPENT_SANT.png}}
	\caption{Schematic representation of the two integrated networks in a tracking-by-detection (TbD) framework.}
	\label{MOT_Framework_Integration_SPENT_SANT}
\end{figure}

\section{Tracking with Prediction and Association Networks}
We apply the tracking-by-detection (TbD) paradigm, in which a tracker fuses object
fuses the object detections to generate object tracks that are consistent over time.
\cite{KF_framework.2013}, for example, provides a framework for analyzing tracking approaches that follow the TbD paradigm. This was used accordingly in this study.

To enable our models to incorporate temporal information, we use Long Short-Term Memory (LSTM) and bidirectional Long Short-Term Memory (BiLSTM) networks. Both for the prediction and the association (SANT) of the sensor objects (SO) to the existing tracks.

\subsection{Prediction Network}
Most existing tracking methods associate incoming detections pairwise with object states predicted by a simple motion model, e.g. a constant velocity model, using a Kalman filter
\cite{KF_simple_cues.2022, KF_simple_online_realtime.2016, KF_framework.2013}.
However, recent work has demonstrated that aggregating temporal information as well as contextual information can improve the tracking of multiple objects by utilizing higher order information in addition to pairwise similarities between detections \cite{DL_RNN_mot.2016, DL_RNN_data_association.2019, DL_RNN_data_association.2020, DL_ATT_CNN_soda.2020}.
Our approach is to use the hidden states of the LSTM layer as an object-specific information storage. 
The Prediction Network was designed for an open-loop application, this means that the network predicts values for a future time step based on previously received data.
For use in an online MOT process, the network is therefore able to make a prediction about the most likely next state values using the state values received for a given Statevector of a Sensorobject (SO).

\begin{figure}[htbp]
	\centerline{\includegraphics[width=0.90\linewidth]{figs/SPENT_network_arch.png}}
	\caption{Schematic representation of the generic prediction network structure.}
	\label{SPENT_network_arch}
\end{figure}

\paragraph{Network architecture} 
The internal values (hidden states) of the LSTM layer are updated per time step based on the received measurement data.
This updating process therefore provides a correction over the course of the sequence of each track.
The number of hidden units (cells) corresponds to the amount of information that the layer remembers between time steps, as shown in Figure \ref{SPENT_network_arch}. 
The hidden states can contain information from all the previous time steps, regardless of the sequence length. 
The values of the hidden states of the LSTM layer are updated in each time step based on the received measurement data and the hidden states of the past timestep.
These updates therefore results in an internal correction over the course of the sequence.
%BATCHNORM
In our prediction network, we have placed a batch normalisation layer directly after the LSTM layer. This means that the output of the LSTM layer is normalised before being passed on to the subsequent Relu layer. This helped speed up training and improve convergence by reducing internal covariate shifts \cite{DL_batch_norm.2015}.
%RELU
A Relu layer performs a threshold operation to each element of the input, where any value less than zero is set to zero.
%The Relu function, also known as Rectified Linear Unit, is defined as follows:
%\begin{equation}
%	\text{ReLU}(x) = \max(0, x)
%\end{equation}
%DROPOUT
At training time, a dropout layer randomly sets input elements to zero with a given probability.
This operation effectively changes the underlying network architecture between iterations and helps prevent the network from overfitting \cite{DL_dropout.2014}.
%FC
The Fully Connected (FC) Layers multiplies the input by a weight matrix and then adds a bias vector. As shown in Figure \ref{SPENT_network_arch} all neurons in a FC layer connect to all the neurons in the previous layer. This layer combines all of the local information learned by the previous layers. 
The last FC layer must be equal to the number of response variables in the Regression Output layer \cite{DL_FC.2010}.
%REGRESSION OUT
The regression layer computes the half-mean-squared-error loss. 
On the output of the regression layer at the end of the network ("regressionoutput"), the network calculates the loss through the mean square error (MSE) for the prediction for each state value.
The MSE indicates the average of the squared difference between the model prediction and the target value, which we
use as the measure of the quality of the prediction. For a single observation, the MSE is given by:
\begin{equation}
	MSE = \sum_{i=1}^{R}\frac{{(gt_i-y_i)}^2}{R}
\end{equation}
where $R$ is the size of the predicted state vector, $gt_i$ is the ground truth value (Kitti car tracks) and $y_i$ is the prediction of the network for sample $i$.
For our sequence-to-sequence regression network the loss function of the regression layer is half the mean square error of the predictions for each time step, normalized by the sequence length $S$: 
\begin{equation}
	loss = \frac{1}{2S} \sum_{i=1}^{S}\sum_{j=1}^{R}{({gt_{ij}}-{y_{ij}})}^2.
\end{equation}
During training, the average loss is calculated using the observations in the mini-batch, so $S$ equals the mini-batch length. %MD: So you dont have a sequence length anymore. So nothing is add up if there are no prediction on certain time stamps.
% MD: More specific: "In relation", do you mean you calculated the RMSE with the same formular? Also: What does this mean? Is is good, is it bad? Will you talk about it later?
In relation to the KITTI data set (cars and vans), a Root Mean Square Error (RMSE) of 0.025 was achieved for the position prediction of all objects in the dataset.
Using a standard Kalman filter (KF), an RMSE of 0.066 was achieved on the same data set.


\paragraph{Data preprocessing}
Ground truth (GT) data in the form of tracks of vehicles from the KITTI data set was used for our development.
A GT Track contains the information of an object over time, starting with its appearance and ending with its exit from the sensor detection range.
In order to achieve better generalization and to increase the chance that the training converges, the track state values at time t (predictors) and track state values at time t+1 (targets) were normalized following
\cite{DL_book.2019}, such that the possible predictions and targets have a mean value of zero and a unit variance.
The mean value $\mu$ and the standard deviation $\sigma$ for each state variable were calculated for all tracks using the following equations:
\begin{equation}
	\mu = \frac{1}{N} \sum_{i = 1}^{N} A_i
\quad\text{and}\quad
	\sigma = \sqrt{{\frac{1}{N - 1}} \sum_{i = 1}^{N} |A_i - \mu|^2}
\end{equation}

where $A_i$ the combined length of all tracks and $N$ is the number of states. %MD: Or time 
%stamps? Even for me this is not exactly clear what is being normalized and how. %CH: added explanation
We also apply pre-padding as described in \cite{DL_padding.2019} where Reddy et al. show how padding influences the performance of neural networks in sequence-based tasks. 
Their study suggests that although both pre-padding and post-padding are feasible, the choice of padding technique can have a significant impact on the efficiency of the model, especially for LSTM networks where the sequence context is crucial. 
%MD: And what does this mean for us? You say it is crucial and then it is not described any further. How did you apply padding? &CH: added text and figure
As \cite{DL_padding.2019} described, padding can lead to noise in the network, but one sequence adjustment per mini-batch is required for training LSTM networks. 
To minimise the effect, the sequences of the training data were therefore sorted by length before the mini-batch padding. Figure \ref{figure_padding} clearly shows that the padding (turquoise area) is minimised by a sorted training data set per mini-batch. Without this modification, no convergence could be achieved in the training.
\begin{figure}[htbp]
	\centerline{\includegraphics[width=0.8\linewidth]{figs/padding.png}}
	\caption{Mini-batch padding effect, training data for LSTM networks.}
	\label{figure_padding}
\end{figure}

%%%%%%%%%%%%%%%%%%%%%% Absatztitel: SANT
\subsection{Association network for a single sensor object}
The association network enables a data-based approach to solve the combinatorial Non Deterministic Polynomial Time (NP) hard optimisation problem of data association.
Compared to most association methods \cite{DL_RNN_mot.2016, DL_RNN_data_association.2019}, the single association network (SANT) is an approach that does not receive a defined distance matrix as input data, but instead receives the existing tracks and each new measured sensor object (SO).

This eliminates the need to define a distance measure, giving the network the freedom to follow its own association logic and learn it based on the training data.
Therefore, the association network replaces the calculation of a distance metric and an association algorithm such as the Hungarian Algorithm (HA) \cite{DA_hungarian.1955}.

\paragraph{Data preprocessing}
In order to develop the single association network (SANT), the data association was considered as a time-structured problem (sequence-to-vector).
The data association problem is therefore the assignment of exactly one sensor object (SO) $Z_{(t,1)}$ to a set of tracks $X_{(t,1:n)}$.
The corresponding tracks were extracted from the KITTI data set of the labelled camera recordings.
However, no real GT data exist for the association problem under consideration.
To ensure that the assignment of a sensor object (SO) to a track of a track set has exactly one correct solution, the SO was generated in a time step from the existing track set of a time step.
The data was noised artificially in order to generate realistic sensor data from the GT data.
All existing GT tracks were noised one by one for each time step.
To achieve this, a maximum of 3\% of the value of the current state vector was randomly subtracted or added to each value. 
The data set was created in 7 iterations, so that the noise intensity was increased by 0.5\% per iteration.

As shown in figure  \ref{SANT_network_arch} the data format was created accordingly to enable index-based track assignment for the Single Association Network (SANT).
The size of the input matrix therefore corresponds to $m * n + 1$. 
With $m = 5$ (number of state values in this investigation) and $n(t)$ (current number of tracks per time step).
The number of tracks currently existing in the time step can vary between 0 and a maximum of 16 objects in relation to the data set (KITTI) and the selected objects (cars and vans).

\begin{figure}[htbp]
	\centerline{\includegraphics[width=1.0\linewidth]{figs/SANT_network_arch.png}}
	\caption{Schematic representation of the generic network structure of SANT.}
	\label{SANT_network_arch}
\end{figure}

\paragraph{Network architecture}
The network ( fig. \ref{SANT_network_arch}) is designed as a sequence-to-classification network, using the sequenceinput to represent the described input matrix per time step.

By combining the outputs of two LSTM layers that pass the information in opposite directions, \cite{DL_LSTM.1997} demonstrates the ability to capture the context from both ends of the sequence. 
The resulting architecture is called Bidirectional / Bidirectional Long Short-Term Memory Network (biLSTM).
As part of these investigations, the best association performance was achieved with the biLSTM layer.
Architectures with RNN, GNU, LSTM and biLSTM layers were tested.
The network shown in Figure \ref{SANT_network_arch} achieved the best performance in comparison with a Training and Validation Accuracy of 95\%.
The output mode 'last' has been configured in the biLSTM layer. 
This layer is therefore able to receive a sequence as input and output a single value or value vector. 
This form of dimension reduction is necessary in order to carry out a corresponding classification.

% FC SOFTMAX STACK
The last fully connected layer (FC) specifies the number of classes via the number of output values. 
The classes are calculated in the Softmax layer by applying the Softmax function
into a unique probability distribution.
The softmax function converts a number of values $z_i$ into a probability vector
with $i$ values. 
A high numerical value leads to a high probability in the resulting output vector (basis for one-hot encoding). 
The outstanding feature of this function is that the sum of the output values (probability values) is always less than or equal to 1:
\begin{equation}
	g(z_i) = \frac{e^{z_i}}{\sum_{j=1}^{n} e^{z_i}} 
\end{equation}

By using the cross-entropy operation in the output layer, SANT could be trained to a correct assignment value (GT class index).
The cross-entropy cost function (cross-entropy) calculates the cross-entropy loss between network predictions and target values for the unique assignment task (for mutually exclusive classes):
One-hot coding is used to represent the class in binary form in a vector and thus generate a 1-to-n code.
The following formula is used to calculate the cross-entropy loss values for each input value $Y_j$ and associated target value $T_j$ element by element:
\begin{equation}
	loss_j = -(T_j ln Y_j + (1 - T_j) ln(1 - Y_j))
\end{equation}
To obtain a scalar $loss$, all loss values $loss_j$ are totalled and divided by the number of samples $N$. 
Optionally, the loss values of defined samples can be weighted with a weighting factor $w_j$:
\begin{equation}
	loss = \frac{1}{N} \sum_{}loss_j w_j
\end{equation}

Eine entsprechende Nutzung des Gewichtungsfaktors $w_j$ kann bei Datensätzen mit unausgewogenen (imbalanced) Klassenverteilung hilfreich sein.



%%%%%%%%%%%%%%%%%%%%%% Absatztitel: MANTa
\subsection{Multi Association Network (MANTa)}
The developed single association network (SANT) shows that a data-based association logic can be learnt from a deep learning model. 
The aim was also to develop a network to recognise a number of m
sensor objects SO to an existing number of n tracks in one operation step.
in one operation step. 
A multi association network (MANTa) was developed, which is able to solve the following association problems:
\begin{itemize}
	\item \textbf{1 to n} - one SO to $n$ tracks
	\item \textbf{m to 1} - $m$ SO to one track
	\item \textbf{m to n} - $m$ SO to $n$ tracks
	\item \textbf{m to 0} - $m$ SO to no tracks
	\item \textbf{0 to n} - no SO to $n$ tracks
	\item \textbf{0 to 0} - no SO to no tracks
\end{itemize}
With the integration of a multi association network (MANTa) into a Multi Object Tracking (MOT) framework, the question can be asked whether a 0 to n and 0 to 0 assignment is a task to be solved. 
If no new SO is detected, the prediction of the last operation step can be continued until the track is deleted on the basis of the decreasing probability of existence within the track management module. 
The association algorithm or the MANTa does not need to be called if no tracks and sensor objects are detected. 
Although these assignment options can therefore be resolved via the programme structure in the MOT framework, these options are also taken into account. 
This is intended to ensure that the network also learns to deal with SOs and tracks that are no longer available.

\paragraph{Datenvorverarbeitung}
Der Datensatz für das Training und die Validierung von MANTa wurde entsprechend der beschriebenen Zielsetzung erstellt. 
Abbildung \ref{pic::MANTaData} zeigt die Inputdatenstruktur mit entsprechenden Zuordnungsaufgaben im dargestellten Zeitschritt (85) der Sequenz 20 des KITTI Datensatzes. 
Über alle Sequenzen wurden die jeweiligen Tracks pro Zeitschritt extrahiert.
Die extrahierten Tracks wurden wie bei der SANT Entwicklung jeweils mit einem Rauschen modifiziert und im Anschluss normiert.
Die so erhaltenen Werte wurden wie in Abb. \ref{pic::MANTaData} dargestellt als Messungen in pseudo-zufälliger Reihenfolge pro Zeitschritt einer $F * T_{max}$ Matrix zugeordnet.
Wobei $F$ die Anzahl der Feature darstellt (hier 10).
Die Featureanzahl ergibt sich aus den Zustandswerten pro Track und SO Set.
$T_{max}$ steht für die maximale Anzahl an existierenden Tracks pro Zeitschritt.
Für den definierten Untersuchungsfall mit Cars und Vans ergibt sich aus dem KITTI Datensatz ein $T_{max} = 16$.
Im dargestellten Zeitschritt der Sequenz existieren 7 Tracks.
Jeder Track erhält in diesem Zeitschritt eine neue Messung, zusätzlich wurde ein weiteres Objekt entdeckt (neues sensor object).
Die GT Zuordnung ist im unteren Teil des Ausschnitts ausgegeben.
Die Werte zeigen den korrekten Index, der Zuordnung für 1 bis 16 ($T_{max}$) bezogen auf das existierende Trackset. 
Der Wert 99 wurde als Klassenindex für neue bzw. nicht zuzuordnende Messungen definiert.
Entsprechend ist die Messung in der zweiten Spalte ein neues Objekt und sollte keinem bestehenden Track zugeordnet werden.

\begin{strip} % <--- defined in "cuted"
		\includegraphics[width=\linewidth]{figs/MANTa_data.png}
		\captionof{figure}{MANTa, data structure, input / output}
		\label{pic::MANTaData}
\end{strip}

\paragraph{Netzwerkentwicklung}

\section{Experimental Evaluation}
... KITTI-Car Benchmark.


\section{Conclusion}
The conclusion goes here.


\section*{Acknowledgments}
This should be a simple paragraph before the References to thank those individuals and institutions who have supported your work on this article.


{\appendix[Proof of the Zonklar Equations]
\section*{Proof of the First Zonklar Equation}
Appendix goes here.
\section*{Proof of the Second Zonklar Equation}
And here.
}

\bibliographystyle{IEEEtran}
\bibliography{IEEEabrv, lit.bib}

\vfill

\end{document}


